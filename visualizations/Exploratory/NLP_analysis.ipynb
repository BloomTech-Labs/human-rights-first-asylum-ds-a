{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\n",
    "\n",
    "*   Text classification \n",
    "*   Represent text in meaningful way\n",
    "    *   labelled inputs (supervised learning)\n",
    "    *   multi-classification model (unsupervised learning)\n",
    "*    Prediction based on classification patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different approaches to text mining: data mining, machine learning, information retrieval and knowledge management.  Each seeks to extract, identify and use information from large collections of textual data.  \n",
    "\n",
    "Text classification is a learning process of text mining.  In this use case it involves preprocessing the data,  weighting terms, using the KNN algorithm in combination with  the K-means clustering algorithim.\n",
    "\n",
    "Evaluation of this classification methodology will be assessed using precisin, recall, and f-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Methodology](assets/HRF_predictive_descriptive_analysis_model_methodology_RJProctor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: until we have data documents to process, this will serve only as an option for future teams - inherited DB from Labs29 is empty with no schema\n",
    "\n",
    "[Labs 29  HRF Asylum B DS AWS RDS PostgresSQL](https://master:***@asylum.catpmmwmrkhp.us-east-1.rds.amazonaws.com/asylum)\n",
    "\n",
    "[Labs 29 AWS Asylum A DS RDS PostgresSQL](https://master:***rds_endpoint=hrfasylum-database-a.catpmmwmrkhp.us-east-1.rds.amazonaws.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Model and  Visualizations \n",
    "\n",
    "The purpose of the model is to classify the legal documents containing judicial decisions for immigration asylum seekers and gain insights from these documents that will aid representatives to advocate for thier client:\n",
    "\n",
    "1.   Judicial decision\n",
    "  *   Asylum Granted\n",
    "  *   Asylum Relief Denied\n",
    "  *   Other Relief Granted\n",
    "  *   Admin Closure (expired)\n",
    "\n",
    "2.   Insights from patterns in data of individual judges (IJ cases only - initial hearings)\n",
    "3.   Insights from patterns in data of appellate (panel) judges (BIA cases only - appellate hearings)\n",
    "4.   Insights from patterns in all data (IJ and BIA cases - combined initial and appellate hearings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database dependency\n",
    "!pip install sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dependency\n",
    "!pip install pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# dataframe tools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# natural language processor\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "# wrapper to show progress/time as self checking tool\n",
    "from tqdm import tqd\n",
    "  \n",
    "\n",
    "# key word extraction\n",
    "import textacy\n",
    "import textacy.ke\n",
    "\n",
    "\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "\n",
    "# linear algebra and array/matrices tools\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "\n",
    "\n",
    "# database tools\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pydantic import BaseModel \n",
    "\n",
    "\n",
    "# graphing tools\n",
    "import matplotlib.pyplot as plt\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database URL\n",
    "database_url = input()\n",
    "\n",
    "\n",
    "#connect to database\n",
    "engine = sqlalchemy.create_engin(database_url)\n",
    "connection = engin.connect()\n",
    "\n",
    "\n",
    "#get_url (without password showing)\n",
    "def get_url():\n",
    "    \"\"\"\n",
    "    verify connection to database\n",
    "    return database connection in URL format:\n",
    "\n",
    "    dialect://user:password@host/dbname\n",
    "\n",
    "    password will be hidden with ***\n",
    "    \"\"\"\n",
    "    url_without_password = repr(connection.engin.url)\n",
    "    return {'database_url': url_without_password}\n",
    "\n",
    "\n",
    "# call function to verify connection to DB\n",
    "get_url()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using write function\n",
    "def write_data(df):\n",
    "    tablename = 'mytable'\n",
    "    df.to_sql(tablename, connection, if_exists='append', index=False, method='multi'\n",
    "\n",
    "\n",
    "# call function to create pandas df\n",
    "write_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using read function\n",
    "def read_data(tablename):\n",
    "    \"\"\"\n",
    "    select all unique records from the database\n",
    "    read them into a dataframe\n",
    "    \"\"\"\n",
    "    query = f\"\"\"SELECT DISTINCT * FROM {tablename} LIMIT 5\"\"\"\n",
    "    df = pd.read_sql(query, connection)\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "\n",
    "# call function to read string fields from RDB table(s)\n",
    "read_data('pdfs')\n",
    "read_data('judges')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "\n",
    "1.   Case folding - the process of changing all letters to lowercase\n",
    "2.   tokenizing - the process of reducing a string to its individual; decrease of documents (parsing) into single words (tokens)\n",
    "1.    Filtering - the process of determining important words from its token\n",
    "2.   Stemming or lemmatiation - technique for reducing word to root word; based on stem or form\n",
    "\n",
    "The purpose of preprocessing data  in the form of numerical values is that this data then becomes the source of data to be processed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def createTokens(df):\n",
    "    '''\n",
    "    function to tokenize, lemmatize lowercase\n",
    "    and remove stop words from text\n",
    "    '''\n",
    "    tokens = []\n",
    "    for doc in tqdm(nlp.pipe(df.astype('unicode').values),total=df.size):\n",
    "        if doc.is_parsed:\n",
    "            tokens.append([n.lemma_.lower() for n in doc if (no n.is_punct and not n._isspace and not n._is stop)])\n",
    "        else:\n",
    "            tokens.append('')\n",
    "    return tokens\n",
    "\n",
    "## dependent on scraping tool separating documents into csv file type as currently written\n",
    "raw = pd.read_csv('')\n",
    "  # as the tabular query has not been built yet so we have no filename to insert here\n",
    "tokens = createTokens(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependent on previous code blocks\n",
    "# find relevant keywords\n",
    "text = ''.join(raw.tolist())\n",
    "nlp.max_length = len(text)\n",
    "\n",
    "keywords = []\n",
    "for tokenlist in tqdm(question_tokens):\n",
    "    doc = nlp(''.join(tokenlist))\n",
    "    # extract and rank\n",
    "    extract = textacy.ke.sgrank(doc, ngrams=(1), window_size=2, normalize=None, topn=2, include_pos=['NOUN', 'PROPN'])\n",
    "        for a, b in extract:\n",
    "            keywords.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort unique keywords by frequency\n",
    "resorted = sorted(set(keywords), key=lambda x: keywords.count(x), reverse=True)\n",
    "# most freq unique keywords assigned to bins\n",
    "top20 = resorted[:20]\n",
    "top200 = resorted[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# extract filenames\n",
    "    # assume huge dataset for scalability\n",
    "    # remove root folder\n",
    "    # traverse through index.html to find pattern to remove title\n",
    "\n",
    "\n",
    "# read all the index.html files at once\n",
    "x[0] for x in os.walk(str(os.getcwd()) + '/<foldername>/')]\n",
    "# remove the extra / for the root folder\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "\n",
    "\n",
    "# use re to match pattern of names and titles\n",
    "names = re.findall(' ', text)\n",
    "  # insert pattern  inside quotes once we have access to files\n",
    "court = re.findall(' ', text)\n",
    "  # insert pattern inside quotes once we have acces to files\n",
    "\n",
    "\n",
    "# iterate\n",
    "# read the file from index filenames\n",
    "data = []\n",
    "\n",
    "for i in folders:\n",
    "    file = open(i + '/index.html', 'r')\n",
    "      # file type will change based on scraper conversion\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    # extract title and names\n",
    "    file_name = names\n",
    "    file_court = court \n",
    "\n",
    "    # iterate to next folder\n",
    "    for j in range(len(file_name)):\n",
    "        data.append((str(i) + str(file_name[j]), file_court[j])) \n",
    "\n",
    "# use a conditional to remove\n",
    "if c == False:\n",
    "    file_name = file_name[2:]\n",
    "    c = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Term Frequency – Inverse Document\n",
    "Frequency (TF-IDF) freature weighting is one of the simplest strongest in the industry.  \n",
    "\n",
    "The Term Frequency (TF) method  weights terms based on the frequency the words appears in  a single document.   The higher the TF value of a word in a document, the higher the effect of that term on the document.\n",
    "\n",
    "The Inverse Document Frequency (IDF) is is a weighting method based on the number of words that appear throughout all the documents in a corpus.\n",
    "\n",
    "TD-IDF, in this use case, is a preprocessing dependency for Cosine similariity and clustering thatsupport efficiencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# court - holds description of court instance (ij(initial hearing), bia(appeal))\n",
    "# body\n",
    "# TF-IDF(doc) = (TF-IDF(court) * alpha) +\n",
    "              # (TF-IDF(body) * (1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "# iterate through all the documents storing all the document id's for each word\n",
    "# processed_text is the body of the document\n",
    "for i in range(len(processed_text)):\n",
    "    tokens = processed_text[i]\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            # add to the set since the set exists\n",
    "            DF[word].add(i)\n",
    "        except:\n",
    "            # create a set since the word doesn't have a set yet\n",
    "            DF[word] = {i}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(DF)->unique words\n",
    "# count of total unique words in vocabulary\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "print(DF)\n",
    "\n",
    "\n",
    "# keys of DF\n",
    "total_vocab = [x for x in DF]\n",
    "print(total_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF-IDF\n",
    "tf_idf = {}\n",
    "for i in range(N):\n",
    "    # calculate TF-IDF for body of all docs\n",
    "    tokens_tf_idf = processed_text[i] \n",
    "\n",
    "    # calculate TF-IDF for title of all docs\n",
    "    counter = Counter(tokens + processed_court[i]) \n",
    "\n",
    "    # iterate body TF-IDF for every (doc, token) if  token is in body, \n",
    "    # replace the body(doc, token) value with the value in \n",
    "    # Title(doc, token)\n",
    "    for token in np.unique(tokens):\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log(N/(df + 1))\n",
    "        # multiplythe body with alpha\n",
    "        tf_idf[doc, token] = tf * idf\n",
    "\n",
    "# TF-IDF = body_TF-IDF * body_weight +\n",
    "          # court_TF-IDF * court_weight\n",
    "\n",
    "           # where body_weight + court_weight = 1\n",
    "\n",
    "# returns tuple(doc, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank using matching score\n",
    "# works for short queries - need to consider scalability (cosine similarity)\n",
    "def matchingScore(query):\n",
    "    query_weights = {}\n",
    "    for kye in tf_idf:\n",
    "        if key[1] in tokens:\n",
    "            # key[0]->doc, key[1]->token\n",
    "            query_weights[key[0]] += tf_idf[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Cosine similarity comppares the inner product of space  between two vectors.  It is measured by the cosine of the angle between two vectors pointing along similar paths.  \n",
    "\n",
    "In this use case, it is used to measure measure document sililarity in text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query and documents  to vectors\n",
    "# use np to store document vectors\n",
    "vect = np.zeros((N, total_vocab_size))\n",
    "\n",
    "for i in tf_idf:\n",
    "    # use list of unique tokens to generate index for each token\n",
    "    ind = total_vocab.index(i[1])\n",
    "    vect[i[0]][ind] = tf_idf[i]\n",
    "\n",
    "\n",
    "# tf, df, idf calculations from query and store in np array\n",
    "Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "counter = Counter(tokens)\n",
    "words_count = len(tokens)\n",
    "\n",
    "\n",
    "query_weights = {}\n",
    "\n",
    "\n",
    "for token in np.unique(tokens):\n",
    "    tf = counter[token]/words_count\n",
    "    df = doc_freq(token)N + 1) / (df + 1))\n",
    "\n",
    "\n",
    "# calculate cosign similarity and return maximum k documents\n",
    "CS = np.dot(a, b) / (norm(a) * norm(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbor (KNN) is a supervised learning algorithm that results in new instance query  which is classified by majority KNN category.  The purpose of the KNN algorithm is to classify a new object (document) based on attributes (keywords) and training samples using minimum distances from the query instance to the training samples to determine the K-nearest neighbors.\n",
    "\n",
    "Traditional KNN text classifiers have several limitations.\n",
    "\n",
    "\n",
    "1.   High calculation complexity to find the KNN samples - all the training samples must be calculated\n",
    "1.   Dependence on training set - the  classifier is generated only with the training samples and does not use any additional data\n",
    "2.   no weight difference between samples - doesn't match actual phenomenon where the samples commonly have uneven distribution\n",
    "\n",
    "By combining KNN with K-means, the expected outcome is a reduction in the complex calculation of training set after determining term weighting as describer of document importance in preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## begin with average KNN model and then adapt to our model needs\n",
    "my_data = \n",
    "  # path will be provided when Database access is provided\n",
    "  \n",
    "# set max columns to none to view every column in dataset\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "my_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature type\n",
    "my_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code here until we have access to data and database\n",
    "\n",
    "# convert any category dtype to object and drop the originals \n",
    "# to avoid conflicts in imputaion\n",
    "\n",
    "my_data['judge1'] = my_data['judge'].astype(object, axis=0)\n",
    "my_data['court1'] = my_data['court'].astype(object, axis=0)\n",
    "\n",
    "my_data = my_data.drop(['judge','court'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of categorical columns to iterate over\n",
    "cat_cols = []\n",
    "  # list will be edited when Database schema is finalized\n",
    "\n",
    "# instantiate \n",
    "encoder = OrdinalEncoder()\n",
    "model = KNN()\n",
    "\n",
    "def encode(data):\n",
    "    \"\"\"\n",
    "    function to encode non-null data and\n",
    "    replace encoded data in original data\n",
    "    \"\"\"\n",
    "    # retain only non-null values\n",
    "    non_nulls = np.array(data.dropna())\n",
    "\n",
    "    # reshape data for encoding\n",
    "    model_reshape = non_nulls.reshape(-1,1)\n",
    "\n",
    "    # encode data\n",
    "    model_encoded = encoder.fit_transform(model_reshape)\n",
    "\n",
    "    # assign back encoded data values to non-null values\n",
    "    data.loc[data.non_nulls()] = np.squeeze(model_encoded)\n",
    "\n",
    "    return data\n",
    "\n",
    "# iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(my_data[columns])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute data and convert \n",
    "encode_data = pd.DataFrame(np.round(imputer.fit_transform(impute_data)),columns = impute_data.columns)\n",
    "    # if you prefer to use the remaining dat as an array, leave out the pd.DataFram() call\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# make document X to be same text feature form as training samples\n",
    "# calculate the similarities between all training samples and document X\n",
    "# choose k samples whaich are larger than N similarities and treat them as a KNN collection\n",
    "# calculate the probability of X belonging toeach category respectively \n",
    "# judge doc X to be the category which has the largest cosign similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using K-means\n",
    "Due to the numbers of calculations taken between samples (test and all of the training samples), traditional KNN has great calculation complexity and can be less efficient with larger datasets.\n",
    "\n",
    "Considering scalibility, combine KNN samples which have largest similarities with clustering technique in order to overcome calculation complexity.\n",
    "\n",
    "The purpose of combining K-means with KNN  is to reduce the time for calculating similarities in the KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## begin with average K-means model and then adapt to our model needss\n",
    "my_data = \n",
    "  # path will be provided when Database access is provided\n",
    "\n",
    "my_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the term frequency (TF from TF-IDF) of the 5 types of claims to calculate frequency\n",
    "# within corpus \n",
    "# create list of variables to visualize\n",
    "X = my_data[[]]\n",
    "  # list will be edited when Database schema is finalized\n",
    "\n",
    "# visualize data points\n",
    "plt.scatter[\n",
    "            X['FearOfPersecution'],\n",
    "            X['SeriousPhysicalHarm'],\n",
    "            X['CoercisveTreatment'],\n",
    "            X['InvidiousProsecutionOrPunishment'],\n",
    "            X['EconomicOrOtherPersecution'],\n",
    "            c='viridis'\n",
    "            ]\n",
    "plt.xlable('Claims for Asylum')\n",
    "plt.ylable('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of clusters based off initial visualization\n",
    "k= \n",
    "\n",
    "# select random observations as centroids\n",
    "centroids = X.sample(n=k)\n",
    "\n",
    "# visualize data points\n",
    "plt.scatter[\n",
    "            X['FearOfPersecution'],\n",
    "            X['SeriousPhysicalHarm'],\n",
    "            X['CoercisveTreatment'],\n",
    "            X['InvidiousProsecutionOrPunishment'],\n",
    "            X['EconomicOrOtherPersecution'],\n",
    "            c='viridis'\n",
    "            ]\n",
    "plt.xlable('Claims for Asylum')\n",
    "plt.ylable('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign each object to the group that is closets to centroid\n",
    "# update centroid by calculating average value of cluster\n",
    "# repeat until centroids no longer move (convergence)\n",
    "diff = 1\n",
    "j=0\n",
    "\n",
    "while(diff !=0):\n",
    "    XD = X\n",
    "    i = 1\n",
    "    for index1.row_c in centroids.iterrows():\n",
    "        ED = []\n",
    "        for index2.row_d in XD.iterrows():\n",
    "            d1 = (row_c[' '] - row_d[' ']**2)\n",
    "             # need to use cosine similarity values here ??\n",
    "            d2 = (row_c[' '] - row_d[' ']**2)\n",
    "              # need to use cosine similarity values here ??\n",
    "            d = np.sqrt(d1 + d2)\n",
    "            ED.append(d)\n",
    "        X[i] = ED\n",
    "        i = i + 1\n",
    "\n",
    "    cluster = []\n",
    "\n",
    "    for index, row in X.iterrows():\n",
    "        min_dist = row[1]\n",
    "        pos = 1\n",
    "        for i in range(k):\n",
    "            if row(i + 1) < min_dist:\n",
    "                min_dist = row[i + 1]\n",
    "                pos = i + 1\n",
    "        cluster.append(pos)\n",
    "\n",
    "    X['Cluster'] = cluster\n",
    "\n",
    "    centroids_new = X.groupby(['Cluster']).mean()[[]]\n",
    "\n",
    "    if j == 0:\n",
    "        diff = 1\n",
    "        j = j + 1\n",
    "    else:\n",
    "        diff = (centroids_new[' ']-centroids[' ']),sum() + (centroids_new[' ']-centroids[' ']),sum()...\n",
    "        print(diff.sum())\n",
    "    \n",
    "    centroids = X.groupby(['Cluster']).mean()[[]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# calculate weight of each document - sum all term weights and divide by total terms in each document\n",
    "# use cosine similarity as weight\n",
    "\n",
    "# cluster each large KNN sample using K-means\n",
    "    # initialize the value of K as the number of clusters of doc to be created\n",
    "    # generate the centroid randomly\n",
    "    # assign each object to the group that is closets to centroid\n",
    "    # update centroid by calculating average value of cluster\n",
    "    # repeat until centroids no longer move (convergence)\n",
    "# after clustering for each category, the cluster centers now represent  the new training sets for KNN algorithm \n",
    "# - reducing time needed for calclating similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore LDA as an alternative to K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# k-fold cross validation ??\n",
    "# create confusion matrix - cluster by system (Y/N), cluster is actually (Y/N)\n",
    "# calculate recall\n",
    "# calculate precision\n",
    "# calculate F-measure ??\n",
    "# inertia ??\n",
    "# dunn index ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar chart\n",
    "specific judge's (historical) grants vs denials of similarly classified cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code until we have real data and database schema\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "  x = [['First', 'First', 'Second', 'Second'],\n",
    "       [\"A\", \"B\", \"A\", \"B\"]],\n",
    "  y = [2, 3, 1, 5],\n",
    "  name = \"Adults\",\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "  x = [['First', 'First', 'Second', 'Second'],\n",
    "       [\"A\", \"B\", \"A\", \"B\"]],\n",
    "  y = [8, 3, 6, 5],\n",
    "  name = \"Children\",\n",
    "))\n",
    "\n",
    "fig.update_layout(title_text=\"Multi-category axis\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code until we have data and database schema\n",
    "df = px.data.tips()\n",
    "fig = px.bar(df, x=\"day\", y=\"total_bill\", color=\"smoker\", barmode=\"group\", facet_col=\"sex\",\n",
    "             category_orders={\"day\": [\"Thur\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "                              \"smoker\": [\"Yes\", \"No\"],\n",
    "                              \"sex\": [\"Male\", \"Female\"]})\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line graph(s)\n",
    "(historical) term frequency associated with specific judge and denials\n",
    "(including appeallate data)\n",
    "(excluding appeallate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code until we have data and database schema\n",
    "df = px.data.gapminder()\n",
    "all_continents = df.continent.unique()\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Checklist(\n",
    "        id=\"checklist\",\n",
    "        options=[{\"label\": x, \"value\": x} \n",
    "                 for x in all_continents],\n",
    "        value=all_continents[3:],\n",
    "        labelStyle={'display': 'inline-block'}\n",
    "    ),\n",
    "    dcc.Graph(id=\"line-chart\"),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"line-chart\", \"figure\"), \n",
    "    [Input(\"checklist\", \"value\")])\n",
    "def update_line_chart(continents):\n",
    "    mask = df.continent.isin(continents)\n",
    "    fig = px.line(df[mask], \n",
    "        x=\"year\", y=\"lifeExp\", color='country')\n",
    "    return fig\n",
    "\n",
    "app.run_server(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code until we have data and database schema \n",
    "df = \n",
    "\n",
    "fig = px.line(df, x='Date', y='AAPL.High', title='Time Series with Range Slider and Selectors')\n",
    "\n",
    "fig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble chart\n",
    "comparison of all current seated judges (historical) grants vs denials of similarly classified cases\n",
    "(or alternative bubble chart to be determined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic code until we have data and database schema\n",
    "# Load data, define hover text and bubble size\n",
    "data = px.data.gapminder()\n",
    "df_2007 = data[data['year']==2007]\n",
    "df_2007 = df_2007.sort_values(['continent', 'country'])\n",
    "\n",
    "hover_text = []\n",
    "bubble_size = []\n",
    "\n",
    "for index, row in df_2007.iterrows():\n",
    "    hover_text.append(('Country: {country}<br>'+\n",
    "                      'Life Expectancy: {lifeExp}<br>'+\n",
    "                      'GDP per capita: {gdp}<br>'+\n",
    "                      'Population: {pop}<br>'+\n",
    "                      'Year: {year}').format(country=row['country'],\n",
    "                                            lifeExp=row['lifeExp'],\n",
    "                                            gdp=row['gdpPercap'],\n",
    "                                            pop=row['pop'],\n",
    "                                            year=row['year']))\n",
    "    bubble_size.append(math.sqrt(row['pop']))\n",
    "\n",
    "df_2007['text'] = hover_text\n",
    "df_2007['size'] = bubble_size\n",
    "sizeref = 2.*max(df_2007['size'])/(100**2)\n",
    "\n",
    "# Dictionary with dataframes for each continent\n",
    "continent_names = ['Africa', 'Americas', 'Asia', 'Europe', 'Oceania']\n",
    "continent_data = {continent:df_2007.query(\"continent == '%s'\" %continent)\n",
    "                              for continent in continent_names}\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for continent_name, continent in continent_data.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=continent['gdpPercap'], y=continent['lifeExp'],\n",
    "        name=continent_name, text=continent['text'],\n",
    "        marker_size=continent['size'],\n",
    "        ))\n",
    "\n",
    "# Tune marker appearance and layout\n",
    "fig.update_traces(mode='markers', marker=dict(sizemode='area',\n",
    "                                              sizeref=sizeref, line_width=2))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Life Expectancy v. Per Capita GDP, 2007',\n",
    "    xaxis=dict(\n",
    "        title='GDP per capita (2000 dollars)',\n",
    "        gridcolor='white',\n",
    "        type='log',\n",
    "        gridwidth=2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Life Expectancy (years)',\n",
    "        gridcolor='white',\n",
    "        gridwidth=2,\n",
    "    ),\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "the team agrees that this model is not appropriate for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear regression model using predictors\n",
    "predictors = []\n",
    "    # update predictors when schema for database is complete\n",
    "    \n",
    "# Split data into predictors X and output Y\n",
    "X = advert[predictors]\n",
    "y = advert['adjudication']\n",
    "\n",
    "# Initialize and fit model\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(X, y)\n",
    "\n",
    "# identify alpha and beta\n",
    "print(f'alpha = {model.intercept_}')\n",
    "print(f'betas = {model.coef_}')\n",
    "\n",
    "# predict judicial outcome\n",
    "model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new predictions\n",
    "new_X = [[_,_]]\n",
    "print(model.predict(new_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adjudication(BaseModule):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--when adding endpoints to ml.py:\n",
    "\n",
    "\n",
    "*   consider null values \n",
    "*   client error/404\n",
    "*   check instance type\n",
    "*   bound integers\n",
    "*   explicitly identify variable object type and allow for conversion\n",
    "*   ***use class method and define function to create json request body***\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f8956436d8ce59f668ff26075b9e4d053dba06b7526eb5909008814682cd7e3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
