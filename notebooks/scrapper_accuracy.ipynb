{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Measure the accuracy of the scrapper based on the hand scraped data from stakeholders\n",
    "## * Just run the cell from top to bottom to see how it works\n",
    "## * It will take a long time to download all the files to your local machine and for the scrapper to scrape data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The cell below is copy straight from ocr.py you can update it with the latest ocr\n",
    "### Need to modify the path under BIA class to get judge_names.pkl\n",
    "### Make_fields func need pages = convert_from_path(file, dpi=90) instead of convert_from_bytes to work in this notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "import json\n",
    "import geonamescache\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from spacy import load\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from typing import List, Iterator\n",
    "import pickle\n",
    "\n",
    "\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def make_fields(uuid, file) -> dict:\n",
    "    \"\"\" This is the main overall function that creates a dictionary of the\n",
    "    desired fields and their respective values; info that goes into those fields.\n",
    "    \"\"\"\n",
    "    # pages = convert_from_bytes(file, dpi=90)\n",
    "    pages = convert_from_path(file, dpi=90)\n",
    "    text = map(pytesseract.image_to_string, pages)\n",
    "    string = \" \".join(text)\n",
    "    case_data = BIACase(uuid, string).to_dict()\n",
    "    return case_data\n",
    "\n",
    "\n",
    "def similar(doc, matcher_pattern):\n",
    "    \"\"\"\n",
    "    A function that uses a spacy Matcher object to search for words or\n",
    "    consecutive words as a phrase.\n",
    "\n",
    "    Format: pattern = [{\"LOWER\": <word>}, {\"LOWER\": <the next word>}, ...etc]\n",
    "    Can look for multiple patterns simultaneously using list of patterns;\n",
    "        [[{\"ARG\": word}], [{\"ARG\": word}], [{\"ARG\": word}]]\n",
    "\n",
    "    DOC: https://spacy.io/usage/rule-based-matching\n",
    "    \"\"\"\n",
    "    # create matcher object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Add the pattern that will be searched for\n",
    "    matcher.add('matcher_pattern', matcher_pattern)\n",
    "\n",
    "    # return the \"matcher\" objects; as Span objects(human readable text)\n",
    "    return matcher(doc, as_spans=True)\n",
    "\n",
    "\n",
    "def similar_outcome(str1, str2):\n",
    "    \"\"\"\n",
    "    Returns True if the strings are off by a single character, and that\n",
    "    character is not a 'd' at the end. That 'd' at the end of a word is highly\n",
    "    indicative of whether something is actually an outcome.\n",
    "\n",
    "    This is used in the get_outcome() method.\n",
    "    \"\"\"\n",
    "    if abs(len(str1) - len(str2)) > 1:\n",
    "        return False\n",
    "    min_len = min(len(str1), len(str2))\n",
    "    i = 0\n",
    "    while i < min_len and str1[i] == str2[i]:\n",
    "        i += 1\n",
    "\n",
    "    # We've reached the end of one string, the other is one character longer\n",
    "    if i == min_len:\n",
    "        # If that character is a 'd', return False, otherwise True\n",
    "        if ((len(str1) > len(str2) and str1[-1] == 'd')\n",
    "                or (len(str2) > len(str1) and str2[-1] == 'd')):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # We're looking at a substitution that is 'd' at the end\n",
    "    if (i == len(str1) - 1 and len(str1) == len(str2)\n",
    "            and (str1[-1] == 'd' or str2[-1] == 'd')):\n",
    "        return False\n",
    "\n",
    "    # We're looking at a substitution other than 'd' at the end\n",
    "    if str1[i + 1:] == str2[i + 1:]:\n",
    "        return True\n",
    "\n",
    "    # We're in the middle, str1 has an extra character\n",
    "    if str1[i + 1:] == str2[i:]:\n",
    "        return True\n",
    "\n",
    "    # We're in the middle, str2 has an extra character\n",
    "    if str1[i:] == str2[i + 1:]:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def in_parenthetical(match):\n",
    "    \"\"\"\n",
    "    Checks for text wrapped in parenthesis, and removes any\n",
    "    returned protected grounds if they we're wrapped in parenthesis\n",
    "    used in protected grounds in order to improve accuracy\n",
    "    \"\"\"\n",
    "    open_parens = 0\n",
    "    # search the rest of the sentence\n",
    "    for i in range(match.end, match.sent.end):\n",
    "        if match.doc[i].text == '(':\n",
    "            open_parens += 1\n",
    "        elif match.doc[i].text == ')':\n",
    "            if open_parens > 0:\n",
    "                open_parens -= 1\n",
    "            else:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class BIACase:\n",
    "    \"\"\"\n",
    "    The following defines the BIACase Class. When receiving a court doc,\n",
    "    we use this to extract info for the desired fields/info\n",
    "    that are scraped from the text of the court docs.\n",
    "    \"\"\"\n",
    "    with open('judge_names.pkl', 'rb') as j:\n",
    "        appellate_panel_members = pickle.load(j)\n",
    "\n",
    "    def __init__(self, uuid: str, text: str):\n",
    "        \"\"\"\n",
    "        • Input will be text from a BIA case pdf file, after the pdf has\n",
    "        been converted from PDF to text.\n",
    "        • Scraping works utilizing spaCy, tokenizing the text, and iterating\n",
    "        token by token searching for matching keywords.\n",
    "        \"\"\"\n",
    "        self.doc: Doc = nlp(text)\n",
    "        self.uuid = uuid\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'uuid': self.uuid,\n",
    "            'panel_members': ', '.join(self.get_panel()) or 'Unknown',\n",
    "            'decision_type': self.get_decision_type() or 'Unknown',\n",
    "            'application_type': self.get_application() or \"Unknown\",\n",
    "            'decision_date': self.get_date() or 'Unknown',\n",
    "            'country_of_origin': self.get_country_of_origin() or 'Unknown',\n",
    "            'outcome': self.get_outcome() or 'Unknown',\n",
    "            'case_origin_state': self.get_state() or 'Unknown',\n",
    "            'case_origin_city': self.get_city() or \"Unknown\",\n",
    "            'protected_grounds': ', '.join(self.get_protected_grounds()) or 'Unknown',\n",
    "            'type_of_persecution': ', '.join(self.get_based_violence()) or 'Unknown',\n",
    "            'gender': self.get_gender() or 'Unknown',\n",
    "            'credibility': str(self.get_credibility()) or 'Unknown',\n",
    "            'check_for_one_year': str(self.check_for_one_year()) or 'Unknown',\n",
    "        }\n",
    "\n",
    "    def get_ents(self, labels: List[str]) -> Iterator[Span]:\n",
    "        \"\"\"\n",
    "        • Retrieves entities of a specified label(s) in the document,\n",
    "        if no label is specified, returns all entities\n",
    "        \"\"\"\n",
    "        return (ent for ent in self.doc.ents if ent.label_ in labels)\n",
    "\n",
    "    def get_country_of_origin(self) -> str:\n",
    "        \"\"\"\n",
    "        RETURNS the respondent's or respondents' country of origin:\n",
    "        \"\"\"\n",
    "        # sorted list of all current countries\n",
    "        gc = geonamescache.GeonamesCache()\n",
    "        countries = sorted(gc.get_countries_by_names().keys())\n",
    "        # remove U.S. and its territories from countries\n",
    "        countries = set(countries)\n",
    "        non_matches = {\"American Samoa\", \"Guam\", \"Northern Mariana Islands\", \"Puerto Rico\", \n",
    "                       \"United States\", \"United States Minor Outlying Islands\", \"U.S. Virgin Islands\"}\n",
    "        countries = countries.difference(non_matches)\n",
    "\n",
    "        \"\"\"\n",
    "        PRIMARY search:\n",
    "        in most cases, the term/pattern \"citizen(s) of\" appears in the same\n",
    "            sentence the country of origin spacy.matcher patterns list, looking\n",
    "            for the following phrase matches following these patterns is\n",
    "            practically guaranteed to be the country of origin\n",
    "        \"\"\"\n",
    "        # create a spacy matcher pattern\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"citizen\"}, {\"LOWER\": \"of\"}],\n",
    "            [{\"LOWER\": \"citizens\"}, {\"LOWER\": \"of\"}],\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            # iterate through the countries list, and return it if it's in the\n",
    "            # cleaned sentence\n",
    "            for country in countries:\n",
    "                if country in clean_sent:\n",
    "                    return country\n",
    "\n",
    "        # SECONDARY search:\n",
    "        # If citizen of wasn't found or if it WAS found but no country followed,\n",
    "        # look through the whole doc for the first instance of a non-U.S. country.\n",
    "        else:\n",
    "            # untokenize and normalize\n",
    "            tok_text = str(self.doc).lower()\n",
    "            # edge case where line breaks appear in the middle of a multi-word\n",
    "            # country, an effect of turning the tokenized text to a string\n",
    "            clean_text = tok_text.replace(\"\\n\", \" \")\n",
    "            # iterate through countries for a foreign entity.\n",
    "            for country in countries:\n",
    "                if country.lower() in clean_text:\n",
    "                    return country\n",
    "\n",
    "    def get_date(self) -> str:\n",
    "        \"\"\"\n",
    "        • Returns decision date of the document.\n",
    "\n",
    "        This is the code to return hearing date \n",
    "        # get_ents function only use in this function\n",
    "        # can be deleted from BIA class if not use \n",
    "\n",
    "        dates = map(str, self.get_ents(['DATE']))\n",
    "        for s in dates:\n",
    "            if len(s.split()) == 3:\n",
    "                return s\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"date\"}, {\"LOWER\": \"of\"}, \n",
    "            {\"LOWER\": \"this\"}, {\"LOWER\": \"notice\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            # iterate through the list of tokens in sentence\n",
    "            # pick out the date in format xxxx/xx/xx\n",
    "            for i in clean_sent.split():\n",
    "                temp = i.split('/')\n",
    "                if len(temp) == 3:\n",
    "                    if len(temp[0]) < 2:\n",
    "                        temp[0] = '0' + temp[0]\n",
    "                    if len(temp[1]) < 2:\n",
    "                        temp[1] = '0' + temp[1]\n",
    "                    result_date = temp[2] + '-' + temp[0] + '-' + temp[1]\n",
    "                    return result_date\n",
    "\n",
    "    def get_panel(self):\n",
    "        \"\"\"\n",
    "        Uses the appellate_panel_members list and spacy PhraseMatcher to check a\n",
    "        document for members in the appellate_panel_member list.\n",
    "        !!! Currently only works for this static list of judges. If not appelate\n",
    "            or the list of apppelate judges changes, or there's an appelate\n",
    "            judge not in the list.\n",
    "            May want to generate an updatable list.\n",
    "            May want to generate a non-appellate judge list\n",
    "            This has important interactions with \"is_appellate()\" function. If\n",
    "                this function returns a judge, it IS from the appellate list,\n",
    "                and is therefore an appellate case.\n",
    "        \"\"\"\n",
    "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        patterns = [nlp.make_doc(text) for text in self.appellate_panel_members]\n",
    "        matcher.add(\"panel_names\", patterns)\n",
    "        matches = set()\n",
    "        for match_id, start, end in matcher(self.doc):\n",
    "            span = self.doc[start:end]\n",
    "            matches.add(' '.join(span.text.split(\", \")[-1::-1]))\n",
    "        return sorted(list(matches))\n",
    "\n",
    "    def get_gender(self) -> str:\n",
    "        \"\"\"\n",
    "        Searches through a given document and counts the TOTAL number of\n",
    "        \"male\" pronoun uses and \"female\" pronoun uses. Whichever\n",
    "        count(\"M\" or \"F\") is higher, that gender is returned.\n",
    "        In the event of a tie; currently returns \"Unknown\"; may be able to\n",
    "        code this edge case. Current accuracy is >95%, low priority fix.\n",
    "        \"\"\"\n",
    "        # List if gendered pronouns\n",
    "        male_prons = ['he', \"he's\", 'his', 'him', 'himself']\n",
    "        female_prons = ['she', \"she's\", 'her', 'hers', 'herself']\n",
    "\n",
    "        # list for spacy.matcher pattens\n",
    "        f_patterns = []\n",
    "        m_patterns = []\n",
    "\n",
    "        # generating list of patterns (f: pattern = [{'LOWER': word}]), for\n",
    "        # spacy matcher search\n",
    "\n",
    "        for prons in female_prons:\n",
    "            f_patterns.append([{'LOWER': prons}])\n",
    "        for prons in male_prons:\n",
    "            m_patterns.append([{'LOWER': prons}])\n",
    "\n",
    "        # use similar() function (Above) to find patterns(pronouns)\n",
    "        m_similar = similar(self.doc, m_patterns)\n",
    "        f_similar = similar(self.doc, f_patterns)\n",
    "\n",
    "        # check the number of gendered pronoun occurrences and return gender\n",
    "        if len(m_similar) > len(f_similar):\n",
    "            return 'Male'\n",
    "        elif len(f_similar) > len(m_similar):\n",
    "            return 'Female'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "    def get_protected_grounds(self):\n",
    "        \"\"\"\n",
    "        This will return the protected ground(s) of the applicant. Special\n",
    "        checks are needed. Checking for keywords is not enough, as sometimes\n",
    "        documents label laws that describe each protected ground. Examples\n",
    "        are 'Purely Political Offense' and 'Real Id Act'.\n",
    "        \"\"\"\n",
    "        pattern = [\n",
    "            [{\"LOWER\": \"race\"}],\n",
    "            [{\"LOWER\": \"religion\"}],  # expand to check for list of religions\n",
    "            [{\"LOWER\": \"nationality\"}],  # phrase is pulled but out of context\n",
    "            [{\"LOWER\": \"social\"}, {\"LOWER\": \"group\"}],\n",
    "            [{\"LOWER\": \"political\"}, {\"LOWER\": \"opinion\"}],\n",
    "            [{\"LOWER\": \"political\"}, {\"LOWER\": \"offense\"}],\n",
    "            [{\"LOWER\": \"political\"}],\n",
    "        ]\n",
    "\n",
    "        religions = ['christianity', 'christian', 'islam', 'atheist',\n",
    "                     'hinduism', 'buddihism', 'jewish', 'judaism', 'islamist',\n",
    "                     'sunni', 'shia', 'muslim', 'buddhist', 'atheists', 'jew',\n",
    "                     'hindu', 'atheism']\n",
    "\n",
    "        politicals = ['political opinion', 'political offense']\n",
    "\n",
    "        confirmed_matches = []\n",
    "        # create pattern for specified religions\n",
    "        for religion in religions:\n",
    "            pattern.append([{\"LOWER\": religion}])\n",
    "\n",
    "        potential_grounds = similar(self.doc, pattern)\n",
    "\n",
    "        for match in potential_grounds:\n",
    "            # skip matches that appear in parenthesis, the opinion is probably\n",
    "            # just quoting a list of all the protected grounds in the statute\n",
    "            if in_parenthetical(match):\n",
    "                continue\n",
    "            # remove 'nationality act' from potential_grounds\n",
    "            if match.text.lower() == 'nationality' \\\n",
    "                    and 'act' not in match.sent.text.lower() \\\n",
    "                    and 'nationality' not in confirmed_matches:\n",
    "                confirmed_matches.append('nationality')\n",
    "\n",
    "            # check for specified religion, replace with 'religion'\n",
    "            elif match.text.lower() in religions:\n",
    "                if 'religion' not in confirmed_matches:\n",
    "                    confirmed_matches.append('religion')\n",
    "\n",
    "            elif match.text.lower() in politicals:\n",
    "                if 'political' not in confirmed_matches:\n",
    "                    confirmed_matches.append('political')\n",
    "\n",
    "            else:\n",
    "                if match.text.lower() not in confirmed_matches:\n",
    "                    confirmed_matches.append(match.text.lower())\n",
    "        return confirmed_matches\n",
    "\n",
    "    def get_application(self) -> str:\n",
    "        \"\"\"\n",
    "        • This will return the seeker's application, found after 'APPLICATION'.\n",
    "        Because HRF is only interested in Asylum, Withholding of Removal,\n",
    "        and Convention Against Torture applications, the others should be\n",
    "        ignored and not included in the dataset.\n",
    "        \"\"\"\n",
    "        app_types = {\n",
    "            'CAT': ['Convention against Torture', 'Convention Against Torture'],\n",
    "            'Asylum': ['Asylum', 'asylum', 'asylum application'],\n",
    "            'Withholding of Removal': ['Withholding of Removal',\n",
    "                                       'withholding of removal'],\n",
    "            'Other': ['Termination', 'Reopening', \"Voluntary Departure\",\n",
    "                      'Cancellation of removal', 'Deferral of removal']\n",
    "        }\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        for token in self.doc:\n",
    "            if token.text == 'APPLICATION':\n",
    "                start += token.idx\n",
    "                break\n",
    "\n",
    "        outcome = set()\n",
    "        for k, v in app_types.items():\n",
    "            for x in v:\n",
    "                if x in self.doc.text[start: start + 300]:\n",
    "                    if k == \"Other\":\n",
    "                        outcome.add(x)\n",
    "                    else:\n",
    "                        outcome.add(k)\n",
    "        return \"; \".join(list(outcome))\n",
    "\n",
    "    def get_decision_type(self) -> str:\n",
    "        return \"Appellate\" if len(self.get_panel()) > 1 else \"Initial\"\n",
    "\n",
    "    def get_outcome(self) -> str:\n",
    "        \"\"\"\n",
    "        • Returns the outcome of the case. This will appear after 'ORDER'\n",
    "        at the end of the document.\n",
    "        \"\"\"\n",
    "        outcomes = {\n",
    "            'remanded',\n",
    "            'reversal',\n",
    "            'dismissed',\n",
    "            'sustained',\n",
    "            'terminated',\n",
    "            'granted',\n",
    "            'denied',\n",
    "            'returned',\n",
    "        }\n",
    "        for token in self.doc:\n",
    "            if token.text in {\"ORDER\", 'ORDERED'}:\n",
    "                start, stop = token.sent.start, token.sent.end + 280\n",
    "                outcome = self.doc[start:stop].text.strip().replace(\"\\n\", \" \")\n",
    "                outcome = outcome.split('.')[0].lower()\n",
    "                for result in outcomes:\n",
    "                    if result in outcome:\n",
    "                        return result.title()\n",
    "\n",
    "    def get_state(self) -> str:\n",
    "        \"\"\"\n",
    "        get_state: Get the state of the original hearing location\n",
    "        Find the \"File:\" pattern in the document and after that\n",
    "        pattern is the State \n",
    "\n",
    "        Returns: The name of the state\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Previous code to find state defeciency\n",
    "        for place in self.doc:\n",
    "            place = place.text\n",
    "            if place in StateLookup.states.keys():\n",
    "                return place\n",
    "            elif place in StateLookup.states.values():\n",
    "                return StateLookup.abbrev_lookup(place)\n",
    "        return \"Unknown\"\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"file\"}, {\"LOWER\": \":\"}],\n",
    "            [{\"LOWER\": \"files\"}, {\"LOWER\": \":\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            try:\n",
    "                state = clean_sent.split(',')[1].split()[0].strip()\n",
    "                return state\n",
    "            except:\n",
    "                return \"Unknown\"\n",
    "            \n",
    "        return \"Unknown\"\n",
    "\n",
    "    def get_city(self) -> str:\n",
    "        \"\"\"\n",
    "        get_city: Get the state of the original hearing location\n",
    "        Find the \"File:\" pattern in the document and after that\n",
    "        pattern is the City \n",
    "\n",
    "        Returns: The name of the city\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"file\"}, {\"LOWER\": \":\"}],\n",
    "            [{\"LOWER\": \"files\"}, {\"LOWER\": \":\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            try:\n",
    "                city = clean_sent.split(',')[0].split()[-1].strip()\n",
    "                return city\n",
    "            except:\n",
    "                return \"Unknown\"\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def get_based_violence(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of keyword buckets which indicate certain types of\n",
    "        violence mentioned in a case, current buckets are: Violence, Family,\n",
    "        Gender, and Gangs. These keywords can be changed in their respective\n",
    "        lists, and an item being present in the list means that the given type\n",
    "        of violence is mentioned in the document.\n",
    "        \"\"\"\n",
    "\n",
    "        # Converts words to lemmas & inputs to nlp-list, then searches for matches in the text\n",
    "        def get_matches(input_list, topic, full_text):\n",
    "            temp_matcher = PhraseMatcher(full_text.vocab, attr=\"LEMMA\")\n",
    "            for n in range(0, len(input_list)):\n",
    "                input_list[n] = nlp(input_list[n])\n",
    "            temp_matcher.add(topic, input_list)\n",
    "            temp_matches = temp_matcher(full_text)\n",
    "            return temp_matches\n",
    "\n",
    "        # Lists of keywords that fall within a bucket to search for\n",
    "        terms_list = []\n",
    "        violent_list = ['abduct', 'abuse', 'assassinate', 'assault', 'coerce',\n",
    "                        'exploit', 'fear', 'harm', 'hurt', 'kidnap', 'kill',\n",
    "                        'murder', 'persecute', 'rape', 'scare', 'shoot',\n",
    "                        'suffer', 'threat', 'torture']\n",
    "        family_list = ['child', 'daughter', 'family', 'husband', 'parent',\n",
    "                       'partner', 'son', 'wife', 'woman']\n",
    "        gender_list = ['fgm', 'gay', 'gender', 'homosexual', 'homosexuality',\n",
    "                       'lesbian', 'lgbt', 'lgbtq', 'lgbtqia',\n",
    "                       'queer', 'sexuality', 'transgender']\n",
    "        gang_list = ['cartel', 'gang', 'militia']\n",
    "\n",
    "        # Outputs a list of PhraseMatch occurrences for a given list of keywords\n",
    "        violence_match = get_matches(violent_list, 'Violent', self.doc)\n",
    "        family_match = get_matches(family_list, 'Family', self.doc)\n",
    "        gender_match = get_matches(gender_list, 'Gender', self.doc)\n",
    "        gang_match = get_matches(gang_list, 'Gang', self.doc)\n",
    "\n",
    "        # Printing full_text[judge_match2[0][1]:judge_match2[0][2]] gives word\n",
    "        # it matches on, can put in the [0] a for loop to see all matches\n",
    "        if len(violence_match) != 0:\n",
    "            terms_list.append('Violent')\n",
    "        if len(family_match) != 0:\n",
    "            terms_list.append('Family')\n",
    "        if len(gender_match) != 0:\n",
    "            terms_list.append('Gender')\n",
    "        if len(gang_match) != 0:\n",
    "            terms_list.append('Gang')\n",
    "        return terms_list\n",
    "\n",
    "    def get_credibility(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns the judge's decision on whether the applicant is a credible witness.\n",
    "        The process starts by adding rules/phrases to SpaCy's Matcher, they were obtained by manually \n",
    "        parsing through case files and finding all sentences related to credibility. \n",
    "        There are three separate rules, narrow, medium and wide, which decrease in the phrasing\n",
    "        specificity, this allows for some wiggle room as opposed to searching for exact matches. \n",
    "        All instances of a match are returned by Matcher, so checking whether these objects are empty \n",
    "        or not dictates the output of this function.\n",
    "        \"\"\"\n",
    "        # # Speciifying phrase patterns / rules to use in SpaCy's Matcher\n",
    "        narrow_scope = [[{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"},\n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"generally\"},\n",
    "                         {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"},\n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"testimony\"},\n",
    "                         {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"}, \n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"credible\"}]]\n",
    "\n",
    "        medium_scope = [[{\"LOWER\": \"credible\"}, {\"LOWER\": \"witness\"}],\n",
    "                        [{\"LOWER\": \"generally\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"internally\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"sufficiently\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"testified\"}, {\"LOWER\": \"credibly\"}],\n",
    "                        [{\"LOWER\": \"testimony\"}, {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"testimony\"}, {\"LOWER\": \"consistent\"}]]\n",
    "\n",
    "        wide_scope = [[{\"LEMMA\": {\"IN\": [\"coherent\", \n",
    "                                        \"possible\", \n",
    "                                        \"credible\", \n",
    "                                        \"consistent\"]}}]]\n",
    "\n",
    "        similar_narrow = similar(self.doc, narrow_scope)\n",
    "        similar_medium = similar(self.doc, medium_scope)\n",
    "        similar_wide = similar(self.doc, wide_scope)\n",
    "\n",
    "        # output logic checks wheteher similar_***** variables are empty or not\n",
    "        # output logic checks whether similar size variables are empty or not\n",
    "        if similar_narrow:\n",
    "            return True\n",
    "\n",
    "        elif similar_medium and similar_wide:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def check_for_one_year(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether or not the asylum-seeker argued to be exempt from the\n",
    "        one-year guideline.\n",
    "\n",
    "        Returns true if the phrases \"within one-year\", \"untimely application\",\n",
    "        \"extraordinary circumstances\" or \"changed circumstances\" appear in the\n",
    "        same sentence as a time-based word. Otherwise returns False.\n",
    "        \"\"\"\n",
    "        time_terms = {'year', 'delay', 'time', 'period', 'deadline'}\n",
    "        # The 'OP':'?' notation means this token is optional, it will match\n",
    "        # sequences with the token and without the token.\n",
    "        circumstance_pattern = [\n",
    "            [{'LEMMA': {'IN': ['change', 'extraordinary']}},\n",
    "             {'LOWER': {'IN': ['\"', '”']}, 'OP': '?'},\n",
    "             {'LEMMA': 'circumstance'}]\n",
    "        ]\n",
    "        application_pattern = [\n",
    "            [{'LOWER': 'untimely'}, {'LOWER': 'application'}]\n",
    "        ]\n",
    "        year_pattern = [\n",
    "            [{'LOWER': 'within'}, {'LOWER': {'IN': ['1', 'one']}},\n",
    "             {'LOWER': '-', 'OP': '?'}, {'LOWER': 'year'}]\n",
    "        ]\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        matcher.add('year pattern', year_pattern)\n",
    "        matcher.add('circumstance pattern', circumstance_pattern)\n",
    "        matcher.add('application pattern', application_pattern)\n",
    "        matches = matcher(self.doc, as_spans=True)\n",
    "\n",
    "        for match in matches:\n",
    "            for token in match.sent:\n",
    "                if token.lemma_ in time_terms:\n",
    "                    return True\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keep this for testing out small cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_file = 'data/334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016.pdf'\n",
    "make_fields('334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016', test_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download the files from manually_scrapped.csv for testing to a data folder\n",
    "### You should create a data folder inside notebook folder before run the cell below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests as re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('manually_scrapped.csv')\n",
    "# This will download all the files into data folder\n",
    "for i in range(len(df)):\n",
    "    r = re.get(df['AWS link'][i])\n",
    "    name = df['uuid'][i]\n",
    "    open('data/' + name, 'wb').write(r.content)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting all the files downloaded from aws using get_aws.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# After download files into data folder using get_aws.ipynb\n",
    "import os\n",
    "\n",
    "path = './data/'\n",
    "owd = os.getcwd()\n",
    "os.chdir(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get cases extract using make_fields"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "cases = []\n",
    "for file in sorted(os.listdir()):\n",
    "    # basename = os.path.basename(path+f'file')\n",
    "    # print(basename)\n",
    "    uuid = file\n",
    "    cases.append(make_fields(f'{uuid}', file))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "# Return to original working repo\n",
    "os.chdir(owd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input all cases in to a csv file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "import csv\n",
    "csv_columns = ['uuid','panel_members','decision_type', 'application_type', 'decision_date', \n",
    "'country_of_origin', 'outcome', 'case_origin_state', 'case_origin_city', 'protected_grounds' , \n",
    "'type_of_persecution', 'gender', 'credibility', 'check_for_one_year']\n",
    "dict_data = cases\n",
    "csv_file = \"ocr_scrapped.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in cases:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This arrange_name() does not use anywhere in scrapper accuracy\n",
    "### However keep this to use for rearrange the name order in the original_manually_scrapped.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def arrange_name(name) -> str: \n",
    "    \"\"\"\n",
    "    This function only accepts string\n",
    "    Take in string and rearrange the position\n",
    "    of name to the format First Middle Last\n",
    "    \n",
    "    Return a string of all panel members\n",
    "    \"\"\"\n",
    "    if type(name)==str:\n",
    "        arr = name.split(';')\n",
    "        result = []\n",
    "        for i in arr:\n",
    "            temp = [x.strip() for x in i.split(',')]\n",
    "            result.append(' '.join(temp[::-1]))\n",
    "        return ','.join(sorted(result)).strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start of the scrapper accuracy\n",
    "### The goal is to try to create two dataframes that has the same col name and the data in the same format \n",
    "### Export these dataframes as csv then use Lambdalib to compare similarity between those csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def wrangle_manual(path):\n",
    "    # Get a copy of these dataframe to not messup the original\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    # Drop columns not using anymore    \n",
    "    df = df.drop(columns=['AWS link'])\n",
    "\n",
    "    # Fillna with Unknown\n",
    "    df = df.fillna('Unknown')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_scrape = pd.read_csv('ocr_scrapped.csv')\n",
    "df_manual = wrangle_manual('manually_scrapped.csv')\n",
    "\n",
    "# Drop cols not hand scrappe by stake holders\n",
    "df_scrape.drop(columns=['decision_type', 'gender'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "df_scrape.to_csv('df_scrape.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "df_manual.to_csv('df_manual.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "from LambdaLib.Analysis.csv_similarity import csv_similarity_score\n",
    "\n",
    "score = csv_similarity_score(\"df_scrape.csv\", \"df_manual.csv\")\n",
    "print(score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6226793305806552\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('human-rights-first-asylum-ds-a-bUMUEGIo': pipenv)"
  },
  "interpreter": {
   "hash": "f9a00ff7b4804186bed77c4f59995e5e4a690b80622b30d4fb44d07178b0cbe5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}