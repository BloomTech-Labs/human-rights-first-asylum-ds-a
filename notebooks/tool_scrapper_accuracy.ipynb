{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Measure the accuracy of the scrapper based on the hand scraped data from stakeholders\n",
    "## * Just run the cell from top to bottom to see how it works\n",
    "## * It will take a long time to download all the files to your local machine and for the scrapper to scrape data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The cell below is copy straight from ocr.py you can update it with the latest ocr\n",
    "### Need to modify the path under BIA class to get court_location.json and judge_names.pkl\n",
    "### Make_fields func need pages = convert_from_path(file, dpi=90) instead of convert_from_bytes to work in this notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import json\n",
    "import geonamescache\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from spacy import load\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from typing import List, Iterator\n",
    "import pickle\n",
    "\n",
    "\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def make_fields(uuid, file) -> dict:\n",
    "    \"\"\" This is the main overall function that creates a dictionary of the\n",
    "    desired fields and their respective values; info that goes into those fields.\n",
    "    \"\"\"\n",
    "    # pages = convert_from_bytes(file, dpi=90)\n",
    "    pages = convert_from_path(file, dpi=90)\n",
    "    text = map(pytesseract.image_to_string, pages)\n",
    "    string = \" \".join(text)\n",
    "    case_data = BIACase(uuid, string).to_dict()\n",
    "    return case_data\n",
    "\n",
    "\n",
    "def similar(doc, matcher_pattern):\n",
    "    \"\"\"\n",
    "    A function that uses a spacy Matcher object to search for words or\n",
    "    consecutive words as a phrase.\n",
    "\n",
    "    Format: pattern = [{\"LOWER\": <word>}, {\"LOWER\": <the next word>}, ...etc]\n",
    "    Can look for multiple patterns simultaneously using list of patterns;\n",
    "        [[{\"ARG\": word}], [{\"ARG\": word}], [{\"ARG\": word}]]\n",
    "\n",
    "    DOC: https://spacy.io/usage/rule-based-matching\n",
    "    \"\"\"\n",
    "    # create matcher object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Add the pattern that will be searched for\n",
    "    matcher.add('matcher_pattern', matcher_pattern)\n",
    "\n",
    "    # return the \"matcher\" objects; as Span objects(human readable text)\n",
    "    return matcher(doc, as_spans=True)\n",
    "\n",
    "\n",
    "def similar_outcome(str1, str2):\n",
    "    \"\"\"\n",
    "    Returns True if the strings are off by a single character, and that\n",
    "    character is not a 'd' at the end. That 'd' at the end of a word is highly\n",
    "    indicative of whether something is actually an outcome.\n",
    "\n",
    "    This is used in the get_outcome() method.\n",
    "    \"\"\"\n",
    "    if abs(len(str1) - len(str2)) > 1:\n",
    "        return False\n",
    "    min_len = min(len(str1), len(str2))\n",
    "    i = 0\n",
    "    while i < min_len and str1[i] == str2[i]:\n",
    "        i += 1\n",
    "\n",
    "    # We've reached the end of one string, the other is one character longer\n",
    "    if i == min_len:\n",
    "        # If that character is a 'd', return False, otherwise True\n",
    "        if ((len(str1) > len(str2) and str1[-1] == 'd')\n",
    "                or (len(str2) > len(str1) and str2[-1] == 'd')):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # We're looking at a substitution that is 'd' at the end\n",
    "    if (i == len(str1) - 1 and len(str1) == len(str2)\n",
    "            and (str1[-1] == 'd' or str2[-1] == 'd')):\n",
    "        return False\n",
    "\n",
    "    # We're looking at a substitution other than 'd' at the end\n",
    "    if str1[i + 1:] == str2[i + 1:]:\n",
    "        return True\n",
    "\n",
    "    # We're in the middle, str1 has an extra character\n",
    "    if str1[i + 1:] == str2[i:]:\n",
    "        return True\n",
    "\n",
    "    # We're in the middle, str2 has an extra character\n",
    "    if str1[i:] == str2[i + 1:]:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def in_parenthetical(match):\n",
    "    \"\"\"\n",
    "    Checks for text wrapped in parenthesis, and removes any\n",
    "    returned protected grounds if they we're wrapped in parenthesis\n",
    "    used in protected grounds in order to improve accuracy\n",
    "    \"\"\"\n",
    "    open_parens = 0\n",
    "    # search the rest of the sentence\n",
    "    for i in range(match.end, match.sent.end):\n",
    "        if match.doc[i].text == '(':\n",
    "            open_parens += 1\n",
    "        elif match.doc[i].text == ')':\n",
    "            if open_parens > 0:\n",
    "                open_parens -= 1\n",
    "            else:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class BIACase:\n",
    "    \"\"\"\n",
    "    The following defines the BIACase Class. When receiving a court doc,\n",
    "    we use this to extract info for the desired fields/info\n",
    "    that are scraped from the text of the court docs.\n",
    "    \"\"\"\n",
    "    with open('../app/court_locations.json') as f:\n",
    "        court_locs = json.load(f)\n",
    "\n",
    "    with open('judge_names.pkl', 'rb') as j:\n",
    "        appellate_panel_members = pickle.load(j)\n",
    "\n",
    "    def __init__(self, uuid: str, text: str):\n",
    "        \"\"\"\n",
    "        • Input will be text from a BIA case pdf file, after the pdf has\n",
    "        been converted from PDF to text.\n",
    "        • Scraping works utilizing spaCy, tokenizing the text, and iterating\n",
    "        token by token searching for matching keywords.\n",
    "        \"\"\"\n",
    "        self.doc: Doc = nlp(text)\n",
    "        self.uuid = uuid\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'uuid': self.uuid,\n",
    "            'panel_members': ', '.join(self.get_panel()) or 'Unknown',\n",
    "            'decision_type': self.get_decision_type() or 'Unknown',\n",
    "            'application_type': self.get_application() or \"Unknown\",\n",
    "            'date': self.get_date() or 'Unknown',\n",
    "            'country_of_origin': self.get_country_of_origin() or 'Unknown',\n",
    "            'outcome': self.get_outcome() or 'Unknown',\n",
    "            'case_origin_state': self.get_state() or 'Unknown',\n",
    "            'case_origin_city': self.get_city() or \"Unknown\",\n",
    "            'protected_grounds': ', '.join(self.get_protected_grounds()) or 'Unknown',\n",
    "            'type_of_persecution': ', '.join(self.get_based_violence()) or 'Unknown',\n",
    "            'gender': self.get_gender() or 'Unknown',\n",
    "            'credibility': str(self.get_credibility()) or 'Unknown',\n",
    "            'check_for_one_year': str(self.check_for_one_year()) or 'Unknown',\n",
    "        }\n",
    "\n",
    "    def get_ents(self, labels: List[str]) -> Iterator[Span]:\n",
    "        \"\"\"\n",
    "        • Retrieves entities of a specified label(s) in the document,\n",
    "        if no label is specified, returns all entities\n",
    "        \"\"\"\n",
    "        return (ent for ent in self.doc.ents if ent.label_ in labels)\n",
    "\n",
    "    def get_country_of_origin(self) -> str:\n",
    "        \"\"\"\n",
    "        RETURNS the respondent's or respondents' country of origin:\n",
    "        \"\"\"\n",
    "        # sorted list of all current countries\n",
    "        gc = geonamescache.GeonamesCache()\n",
    "        countries = sorted(gc.get_countries_by_names().keys())\n",
    "        # remove U.S. and its territories from countries\n",
    "        countries = set(countries)\n",
    "        non_matches = {\"American Samoa\", \"Guam\", \"Northern Mariana Islands\", \"Puerto Rico\", \n",
    "                       \"United States\", \"United States Minor Outlying Islands\", \"U.S. Virgin Islands\"}\n",
    "        countries = countries.difference(non_matches)\n",
    "\n",
    "        \"\"\"\n",
    "        PRIMARY search:\n",
    "        in most cases, the term/pattern \"citizen(s) of\" appears in the same\n",
    "            sentence the country of origin spacy.matcher patterns list, looking\n",
    "            for the following phrase matches following these patterns is\n",
    "            practically guaranteed to be the country of origin\n",
    "        \"\"\"\n",
    "        # create a spacy matcher pattern\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"citizen\"}, {\"LOWER\": \"of\"}],\n",
    "            [{\"LOWER\": \"citizens\"}, {\"LOWER\": \"of\"}],\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            # iterate through the countries list, and return it if it's in the\n",
    "            # cleaned sentence\n",
    "            for country in countries:\n",
    "                if country in clean_sent:\n",
    "                    return country\n",
    "\n",
    "        # SECONDARY search:\n",
    "        # If citizen of wasn't found or if it WAS found but no country followed,\n",
    "        # look through the whole doc for the first instance of a non-U.S. country.\n",
    "        else:\n",
    "            # untokenize and normalize\n",
    "            tok_text = str(self.doc).lower()\n",
    "            # edge case where line breaks appear in the middle of a multi-word\n",
    "            # country, an effect of turning the tokenized text to a string\n",
    "            clean_text = tok_text.replace(\"\\n\", \" \")\n",
    "            # iterate through countries for a foreign entity.\n",
    "            for country in countries:\n",
    "                if country.lower() in clean_text:\n",
    "                    return country\n",
    "\n",
    "    def get_date(self) -> str:\n",
    "        \"\"\"\n",
    "        • Returns decision date of the document.\n",
    "\n",
    "        This is the code to return hearing date \n",
    "        # get_ents function only use in this function\n",
    "        # can be deleted from BIA class if not use \n",
    "\n",
    "        dates = map(str, self.get_ents(['DATE']))\n",
    "        for s in dates:\n",
    "            if len(s.split()) == 3:\n",
    "                return s\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"date\"}, {\"LOWER\": \"of\"}, \n",
    "            {\"LOWER\": \"this\"}, {\"LOWER\": \"notice\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            # iterate through the list of tokens in sentence\n",
    "            # pick out the date in format xxxx/xx/xx\n",
    "            for i in clean_sent.split():\n",
    "                temp = i.split('/')\n",
    "                if len(temp) == 3:\n",
    "                    if len(temp[0]) < 2:\n",
    "                        temp[0] = '0' + temp[0]\n",
    "                    if len(temp[1]) < 2:\n",
    "                        temp[1] = '0' + temp[1]\n",
    "                    result_date = temp[2] + '-' + temp[0] + '-' + temp[1]\n",
    "                    return result_date\n",
    "\n",
    "    def get_panel(self):\n",
    "        \"\"\"\n",
    "        Uses the appellate_panel_members list and spacy PhraseMatcher to check a\n",
    "        document for members in the appellate_panel_member list.\n",
    "        !!! Currently only works for this static list of judges. If not appelate\n",
    "            or the list of apppelate judges changes, or there's an appelate\n",
    "            judge not in the list.\n",
    "            May want to generate an updatable list.\n",
    "            May want to generate a non-appellate judge list\n",
    "            This has important interactions with \"is_appellate()\" function. If\n",
    "                this function returns a judge, it IS from the appellate list,\n",
    "                and is therefore an appellate case.\n",
    "        \"\"\"\n",
    "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        patterns = [nlp.make_doc(text) for text in self.appellate_panel_members]\n",
    "        matcher.add(\"panel_names\", patterns)\n",
    "        matches = set()\n",
    "        for match_id, start, end in matcher(self.doc):\n",
    "            span = self.doc[start:end]\n",
    "            matches.add(' '.join(span.text.split(\", \")[-1::-1]))\n",
    "        return sorted(list(matches))\n",
    "\n",
    "    def get_gender(self) -> str:\n",
    "        \"\"\"\n",
    "        Searches through a given document and counts the TOTAL number of\n",
    "        \"male\" pronoun uses and \"female\" pronoun uses. Whichever\n",
    "        count(\"M\" or \"F\") is higher, that gender is returned.\n",
    "        In the event of a tie; currently returns \"Unknown\"; may be able to\n",
    "        code this edge case. Current accuracy is >95%, low priority fix.\n",
    "        \"\"\"\n",
    "        # List if gendered pronouns\n",
    "        male_prons = ['he', \"he's\", 'his', 'him', 'himself']\n",
    "        female_prons = ['she', \"she's\", 'her', 'hers', 'herself']\n",
    "\n",
    "        # list for spacy.matcher pattens\n",
    "        f_patterns = []\n",
    "        m_patterns = []\n",
    "\n",
    "        # generating list of patterns (f: pattern = [{'LOWER': word}]), for\n",
    "        # spacy matcher search\n",
    "\n",
    "        for prons in female_prons:\n",
    "            f_patterns.append([{'LOWER': prons}])\n",
    "        for prons in male_prons:\n",
    "            m_patterns.append([{'LOWER': prons}])\n",
    "\n",
    "        # use similar() function (Above) to find patterns(pronouns)\n",
    "        m_similar = similar(self.doc, m_patterns)\n",
    "        f_similar = similar(self.doc, f_patterns)\n",
    "\n",
    "        # check the number of gendered pronoun occurrences and return gender\n",
    "        if len(m_similar) > len(f_similar):\n",
    "            return 'Male'\n",
    "        elif len(f_similar) > len(m_similar):\n",
    "            return 'Female'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "    def get_protected_grounds(self):\n",
    "        \"\"\"\n",
    "        This will return the protected ground(s) of the applicant. Special\n",
    "        checks are needed. Checking for keywords is not enough, as sometimes\n",
    "        documents label laws that describe each protected ground. Examples\n",
    "        are 'Purely Political Offense' and 'Real Id Act'.\n",
    "        \"\"\"\n",
    "        pattern = [\n",
    "            [{\"LOWER\": \"race\"}],\n",
    "            [{\"LOWER\": \"religion\"}],  # expand to check for list of religions\n",
    "            [{\"LOWER\": \"nationality\"}],  # phrase is pulled but out of context\n",
    "            [{\"LOWER\": \"social\"}, {\"LOWER\": \"group\"}],\n",
    "            [{\"LOWER\": \"political\"}, {\"LOWER\": \"opinion\"}],\n",
    "            [{\"LOWER\": \"political\"}, {\"LOWER\": \"offense\"}],\n",
    "            [{\"LOWER\": \"political\"}],\n",
    "        ]\n",
    "\n",
    "        religions = ['christianity', 'christian', 'islam', 'atheist',\n",
    "                     'hinduism', 'buddihism', 'jewish', 'judaism', 'islamist',\n",
    "                     'sunni', 'shia', 'muslim', 'buddhist', 'atheists', 'jew',\n",
    "                     'hindu', 'atheism']\n",
    "\n",
    "        politicals = ['political opinion', 'political offense']\n",
    "\n",
    "        confirmed_matches = []\n",
    "        # create pattern for specified religions\n",
    "        for religion in religions:\n",
    "            pattern.append([{\"LOWER\": religion}])\n",
    "\n",
    "        potential_grounds = similar(self.doc, pattern)\n",
    "\n",
    "        for match in potential_grounds:\n",
    "            # skip matches that appear in parenthesis, the opinion is probably\n",
    "            # just quoting a list of all the protected grounds in the statute\n",
    "            if in_parenthetical(match):\n",
    "                continue\n",
    "            # remove 'nationality act' from potential_grounds\n",
    "            if match.text.lower() == 'nationality' \\\n",
    "                    and 'act' not in match.sent.text.lower() \\\n",
    "                    and 'nationality' not in confirmed_matches:\n",
    "                confirmed_matches.append('nationality')\n",
    "\n",
    "            # check for specified religion, replace with 'religion'\n",
    "            elif match.text.lower() in religions:\n",
    "                if 'religion' not in confirmed_matches:\n",
    "                    confirmed_matches.append('religion')\n",
    "\n",
    "            elif match.text.lower() in politicals:\n",
    "                if 'political' not in confirmed_matches:\n",
    "                    confirmed_matches.append('political')\n",
    "\n",
    "            else:\n",
    "                if match.text.lower() not in confirmed_matches:\n",
    "                    confirmed_matches.append(match.text.lower())\n",
    "        return confirmed_matches\n",
    "\n",
    "    def get_application(self) -> str:\n",
    "        \"\"\"\n",
    "        • This will return the seeker's application, found after 'APPLICATION'.\n",
    "        Because HRF is only interested in Asylum, Withholding of Removal,\n",
    "        and Convention Against Torture applications, the others should be\n",
    "        ignored and not included in the dataset.\n",
    "        \"\"\"\n",
    "        app_types = {\n",
    "            'CAT': ['Convention against Torture', 'Convention Against Torture'],\n",
    "            'Asylum': ['Asylum', 'asylum', 'asylum application'],\n",
    "            'Withholding of Removal': ['Withholding of Removal',\n",
    "                                       'withholding of removal'],\n",
    "            'Other': ['Termination', 'Reopening', \"Voluntary Departure\",\n",
    "                      'Cancellation of removal', 'Deferral of removal']\n",
    "        }\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        for token in self.doc:\n",
    "            if token.text == 'APPLICATION':\n",
    "                start += token.idx\n",
    "                break\n",
    "\n",
    "        outcome = set()\n",
    "        for k, v in app_types.items():\n",
    "            for x in v:\n",
    "                if x in self.doc.text[start: start + 300]:\n",
    "                    if k == \"Other\":\n",
    "                        outcome.add(x)\n",
    "                    else:\n",
    "                        outcome.add(k)\n",
    "        return \"; \".join(list(outcome))\n",
    "\n",
    "    def get_decision_type(self) -> str:\n",
    "        return \"Appellate\" if len(self.get_panel()) > 1 else \"Initial\"\n",
    "\n",
    "    def get_outcome(self) -> str:\n",
    "        \"\"\"\n",
    "        • Returns the outcome of the case. This will appear after 'ORDER'\n",
    "        at the end of the document.\n",
    "        \"\"\"\n",
    "        outcomes = {\n",
    "            'remanded',\n",
    "            'reversal',\n",
    "            'dismissed',\n",
    "            'sustained',\n",
    "            'terminated',\n",
    "            'granted',\n",
    "            'denied',\n",
    "            'returned',\n",
    "        }\n",
    "        for token in self.doc:\n",
    "            if token.text in {\"ORDER\", 'ORDERED'}:\n",
    "                start, stop = token.sent.start, token.sent.end + 280\n",
    "                outcome = self.doc[start:stop].text.strip().replace(\"\\n\", \" \")\n",
    "                outcome = outcome.split('.')[0].lower()\n",
    "                for result in outcomes:\n",
    "                    if result in outcome:\n",
    "                        return result.title()\n",
    "\n",
    "    def get_state(self) -> str:\n",
    "        \"\"\"\n",
    "        get_state: Get the state of the original hearing location\n",
    "        Find the \"File:\" pattern in the document and after that\n",
    "        pattern is the State \n",
    "\n",
    "        Returns: The name of the state\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Previous code to find state defeciency\n",
    "        for place in self.doc:\n",
    "            place = place.text\n",
    "            if place in StateLookup.states.keys():\n",
    "                return place\n",
    "            elif place in StateLookup.states.values():\n",
    "                return StateLookup.abbrev_lookup(place)\n",
    "        return \"Unknown\"\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"file\"}, {\"LOWER\": \":\"}],\n",
    "            [{\"LOWER\": \"files\"}, {\"LOWER\": \":\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            state = clean_sent.split(',')[1].split()[0].strip()\n",
    "            return state\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def get_city(self) -> str:\n",
    "        \"\"\"\n",
    "        get_city: Get the state of the original hearing location\n",
    "        Find the \"File:\" pattern in the document and after that\n",
    "        pattern is the City \n",
    "\n",
    "        Returns: The name of the city\n",
    "        \"\"\"\n",
    "        primary_pattern = [\n",
    "            [{\"LOWER\": \"file\"}, {\"LOWER\": \":\"}],\n",
    "            [{\"LOWER\": \"files\"}, {\"LOWER\": \":\"}]\n",
    "        ]\n",
    "        # instantiate a list of pattern matches\n",
    "        spans = similar(self.doc, primary_pattern)\n",
    "        # if there are matches\n",
    "        if spans:\n",
    "            # grab the surrounding sentence and turn it into a string\n",
    "            sentence = str(spans[0].sent)\n",
    "            # remove line breaks, edge case\n",
    "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
    "            city = clean_sent.split(',')[0].split()[-1].strip()\n",
    "            return city\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def get_based_violence(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of keyword buckets which indicate certain types of\n",
    "        violence mentioned in a case, current buckets are: Violence, Family,\n",
    "        Gender, and Gangs. These keywords can be changed in their respective\n",
    "        lists, and an item being present in the list means that the given type\n",
    "        of violence is mentioned in the document.\n",
    "        \"\"\"\n",
    "\n",
    "        # Converts words to lemmas & inputs to nlp-list, then searches for matches in the text\n",
    "        def get_matches(input_list, topic, full_text):\n",
    "            temp_matcher = PhraseMatcher(full_text.vocab, attr=\"LEMMA\")\n",
    "            for n in range(0, len(input_list)):\n",
    "                input_list[n] = nlp(input_list[n])\n",
    "            temp_matcher.add(topic, input_list)\n",
    "            temp_matches = temp_matcher(full_text)\n",
    "            return temp_matches\n",
    "\n",
    "        # Lists of keywords that fall within a bucket to search for\n",
    "        terms_list = []\n",
    "        violent_list = ['abduct', 'abuse', 'assassinate', 'assault', 'coerce',\n",
    "                        'exploit', 'fear', 'harm', 'hurt', 'kidnap', 'kill',\n",
    "                        'murder', 'persecute', 'rape', 'scare', 'shoot',\n",
    "                        'suffer', 'threat', 'torture']\n",
    "        family_list = ['child', 'daughter', 'family', 'husband', 'parent',\n",
    "                       'partner', 'son', 'wife', 'woman']\n",
    "        gender_list = ['fgm', 'gay', 'gender', 'homosexual', 'homosexuality',\n",
    "                       'lesbian', 'lgbt', 'lgbtq', 'lgbtqia',\n",
    "                       'queer', 'sexuality', 'transgender']\n",
    "        gang_list = ['cartel', 'gang', 'militia']\n",
    "\n",
    "        # Outputs a list of PhraseMatch occurrences for a given list of keywords\n",
    "        violence_match = get_matches(violent_list, 'Violent', self.doc)\n",
    "        family_match = get_matches(family_list, 'Family', self.doc)\n",
    "        gender_match = get_matches(gender_list, 'Gender', self.doc)\n",
    "        gang_match = get_matches(gang_list, 'Gang', self.doc)\n",
    "\n",
    "        # Printing full_text[judge_match2[0][1]:judge_match2[0][2]] gives word\n",
    "        # it matches on, can put in the [0] a for loop to see all matches\n",
    "        if len(violence_match) != 0:\n",
    "            terms_list.append('Violent')\n",
    "        if len(family_match) != 0:\n",
    "            terms_list.append('Family')\n",
    "        if len(gender_match) != 0:\n",
    "            terms_list.append('Gender')\n",
    "        if len(gang_match) != 0:\n",
    "            terms_list.append('Gang')\n",
    "        return terms_list\n",
    "\n",
    "    def get_credibility(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns the judge's decision on whether the applicant is a credible witness.\n",
    "        The process starts by adding rules/phrases to SpaCy's Matcher, they were obtained by manually \n",
    "        parsing through case files and finding all sentences related to credibility. \n",
    "        There are three separate rules, narrow, medium and wide, which decrease in the phrasing\n",
    "        specificity, this allows for some wiggle room as opposed to searching for exact matches. \n",
    "        All instances of a match are returned by Matcher, so checking whether these objects are empty \n",
    "        or not dictates the output of this function.\n",
    "        \"\"\"\n",
    "        # # Speciifying phrase patterns / rules to use in SpaCy's Matcher\n",
    "        narrow_scope = [[{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"},\n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"generally\"},\n",
    "                         {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"},\n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"testimony\"},\n",
    "                         {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"court\"}, {\"LOWER\": \"finds\"}, \n",
    "                         {\"LOWER\": \"respondent\"}, {\"LOWER\": \"credible\"}]]\n",
    "\n",
    "        medium_scope = [[{\"LOWER\": \"credible\"}, {\"LOWER\": \"witness\"}],\n",
    "                        [{\"LOWER\": \"generally\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"internally\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"sufficiently\"}, {\"LOWER\": \"consistent\"}],\n",
    "                        [{\"LOWER\": \"testified\"}, {\"LOWER\": \"credibly\"}],\n",
    "                        [{\"LOWER\": \"testimony\"}, {\"LOWER\": \"credible\"}],\n",
    "                        [{\"LOWER\": \"testimony\"}, {\"LOWER\": \"consistent\"}]]\n",
    "\n",
    "        wide_scope = [[{\"LEMMA\": {\"IN\": [\"coherent\", \n",
    "                                        \"possible\", \n",
    "                                        \"credible\", \n",
    "                                        \"consistent\"]}}]]\n",
    "\n",
    "        similar_narrow = similar(self.doc, narrow_scope)\n",
    "        similar_medium = similar(self.doc, medium_scope)\n",
    "        similar_wide = similar(self.doc, wide_scope)\n",
    "\n",
    "        # output logic checks wheteher similar_***** variables are empty or not\n",
    "        # output logic checks whether similar size variables are empty or not\n",
    "        if similar_narrow:\n",
    "            return True\n",
    "\n",
    "        elif similar_medium and similar_wide:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def check_for_one_year(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether or not the asylum-seeker argued to be exempt from the\n",
    "        one-year guideline.\n",
    "\n",
    "        Returns true if the phrases \"within one-year\", \"untimely application\",\n",
    "        \"extraordinary circumstances\" or \"changed circumstances\" appear in the\n",
    "        same sentence as a time-based word. Otherwise returns False.\n",
    "        \"\"\"\n",
    "        time_terms = {'year', 'delay', 'time', 'period', 'deadline'}\n",
    "        # The 'OP':'?' notation means this token is optional, it will match\n",
    "        # sequences with the token and without the token.\n",
    "        circumstance_pattern = [\n",
    "            [{'LEMMA': {'IN': ['change', 'extraordinary']}},\n",
    "             {'LOWER': {'IN': ['\"', '”']}, 'OP': '?'},\n",
    "             {'LEMMA': 'circumstance'}]\n",
    "        ]\n",
    "        application_pattern = [\n",
    "            [{'LOWER': 'untimely'}, {'LOWER': 'application'}]\n",
    "        ]\n",
    "        year_pattern = [\n",
    "            [{'LOWER': 'within'}, {'LOWER': {'IN': ['1', 'one']}},\n",
    "             {'LOWER': '-', 'OP': '?'}, {'LOWER': 'year'}]\n",
    "        ]\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        matcher.add('year pattern', year_pattern)\n",
    "        matcher.add('circumstance pattern', circumstance_pattern)\n",
    "        matcher.add('application pattern', application_pattern)\n",
    "        matches = matcher(self.doc, as_spans=True)\n",
    "\n",
    "        for match in matches:\n",
    "            for token in match.sent:\n",
    "                if token.lemma_ in time_terms:\n",
    "                    return True\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keep this for testing out small cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "test_file = 'data/334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016.pdf'\n",
    "make_fields('334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016', test_file)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'uuid': '334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016',\n",
       " 'panel_members': 'Anne J. Greer',\n",
       " 'decision_type': 'Initial',\n",
       " 'application_type': 'CAT; Withholding of Removal; Asylum',\n",
       " 'date': '2016-11-04',\n",
       " 'country_of_origin': 'El Salvador',\n",
       " 'outcome': 'Remanded',\n",
       " 'case_origin_state': 'MD',\n",
       " 'case_origin_city': 'Baltimore',\n",
       " 'protected_grounds': 'race, religion, nationality, social group, political',\n",
       " 'type_of_persecution': 'Violent, Family, Gang',\n",
       " 'gender': 'Female',\n",
       " 'credibility': 'False',\n",
       " 'check_for_one_year': 'True'}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download the files from manually_scrapped.csv for testing to a data folder\n",
    "### You should create a data folder inside notebook folder before run the cell below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests as re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('manually_scrapped.csv')\n",
    "# This will download all the files into data folder\n",
    "for i in range(len(df)):\n",
    "    r = re.get(df['AWS link'][i])\n",
    "    name = df['uuid'][i]\n",
    "    open('data/' + name, 'wb').write(r.content)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting all the files downloaded from aws using get_aws.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "# After download files into data folder using get_aws.ipynb\n",
    "import os\n",
    "\n",
    "path = './data/'\n",
    "owd = os.getcwd()\n",
    "os.chdir(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get cases extract using make_fields"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "cases = []\n",
    "for file in sorted(os.listdir()):\n",
    "    # basename = os.path.basename(path+f'file')\n",
    "    # print(basename)\n",
    "    uuid = file\n",
    "    cases.append(make_fields(f'{uuid}', file))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "# Return to original working repo\n",
    "os.chdir(owd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input all cases in to a csv file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "import csv\n",
    "csv_columns = ['uuid','panel_members','decision_type', 'application_type', 'date', \n",
    "'country_of_origin', 'outcome', 'case_origin_state', 'case_origin_city', 'protected_grounds' , \n",
    "'type_of_persecution', 'gender', 'credibility', 'check_for_one_year']\n",
    "dict_data = cases\n",
    "csv_file = \"ocr_scrapped.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in cases:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This arrange_name() does not use anywhere in scrapper accuracy\n",
    "### However keep this to use for rearrange the name order in the original_manually_scrapped.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def arrange_name(name) -> str: \n",
    "    \"\"\"\n",
    "    This function only accepts string\n",
    "    Take in string and rearrange the position\n",
    "    of name to the format First Middle Last\n",
    "    \n",
    "    Return a string of all panel members\n",
    "    \"\"\"\n",
    "    if type(name)==str:\n",
    "        arr = name.split(';')\n",
    "        result = []\n",
    "        for i in arr:\n",
    "            temp = [x.strip() for x in i.split(',')]\n",
    "            result.append(' '.join(temp[::-1]))\n",
    "        return ','.join(sorted(result)).strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start of the scrapper accuracy\n",
    "### The goal is to try to create two dataframes that has the same col name and the data in the same format \n",
    "### Export these dataframes as csv then use Lambdalib to compare similarity between those csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def wrangle_manual(path):\n",
    "    # Get a copy of these dataframe to not messup the original\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    # Drop columns not using anymore    \n",
    "    df = df.drop(columns=['AWS link'])\n",
    "\n",
    "    # Fillna with Unknown\n",
    "    df = df.fillna('Unknown')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_scrape = pd.read_csv('ocr_scrapped.csv')\n",
    "df_manual = wrangle_manual('manually_scrapped.csv')\n",
    "\n",
    "# Drop cols not hand scrappe by stake holders\n",
    "df_scrape.drop(columns=['decision_type', 'gender'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df_manual.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 uuid  \\\n",
       "0   140194281-Ali-Fares-A047-654-200-BIA-Apr-30-20...   \n",
       "1                 165227167-K-O-A-BIA-Aug-27-2013.pdf   \n",
       "2   171952033-Luis-Narciso-Sedeno-Trujillo-A088-19...   \n",
       "3   175361890-Jose-Zacaria-Quinteros-A088-239-850-...   \n",
       "4   202216334-Francisco-Hernandez-Pina-A073-976-63...   \n",
       "5     205871959-M-G-O-AXXX-XXX-611-BIA-Feb-4-2014.pdf   \n",
       "6   208167027-J-M-S-B-W-AXX-XXX-109-BIA-Apr-4-2003...   \n",
       "7   225423441-Roberson-Joseph-A078-360-606-BIA-Nov...   \n",
       "8                 271354416-D-M-R-BIA-June-9-2015.pdf   \n",
       "9   309306110-Sergio-Maldonado-A078-567-541-BIA-Ma...   \n",
       "10  311736830-H-R-M-AXXX-XXX-381-BIA-March-14-2016...   \n",
       "11  318845018-Ennio-Edgardo-Galvez-A095-006-115-BI...   \n",
       "12  322069426-Maricela-Jacquelin-Madrid-Gomez-A206...   \n",
       "13    328162186-H-B-AXXX-XXX-608-BIA-July-26-2016.pdf   \n",
       "14    334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016.pdf   \n",
       "15  337539141-Bertha-A-Lopez-Tovar-A071-904-519-BI...   \n",
       "16  337540716-Clebson-Sousa-Carneiro-A078-254-701-...   \n",
       "17  337542906-A-A-L-M-AXXX-XXX-585-BIA-Oct-22-2015...   \n",
       "18   349320269-S-D-AXXX-XXX-230-BIA-April-26-2017.pdf   \n",
       "19  349321596-Adela-Nolasco-Santiago-A205-497-497-...   \n",
       "\n",
       "                                        panel_members  \\\n",
       "0                                   Michael J. Creppy   \n",
       "1   Linda S. Wendtland,Roger Pauley,Teresa L. Donovan   \n",
       "2       Anne J. Greer,Linda S. Wendtland,Roger Pauley   \n",
       "3       Anne J. Greer,Linda S. Wendtland,Roger Pauley   \n",
       "4         Anne J. Greer,Patricia A. Cole,Roger Pauley   \n",
       "5   Linda S. Wendtland,Roger Pauley,Teresa L. Donovan   \n",
       "6                                     Edward R. Grant   \n",
       "7       Anne J. Greer,Linda S. Wendtland,Roger Pauley   \n",
       "8   Anne J. Greer,Charles K. Adkins-Blanch,Neil P....   \n",
       "9   Brian M. O'Leary,Edward R. Grant,John Guendels...   \n",
       "10  Anne J. Greer,Charles K. Adkins-Blanch,Margare...   \n",
       "11                                    Edward R. Grant   \n",
       "12  Blair O'Connor,Brian M. O'Leary,Charles K. Adk...   \n",
       "13      Anne J. Greer,Linda S. Wendtland,Roger Pauley   \n",
       "14  Anrie J. Greer,Charles K. Adkins-Blanch,Margar...   \n",
       "15     Joan B Geller,John Guendelsberger,Roger Pauley   \n",
       "16   Blair Ana O'Connor Mann Charles K. Adkins-Blanch   \n",
       "17   Margaret M Neil P. O'Herron Anne J. Miller Greer   \n",
       "18  Hugh G. Ellen C Mullane Michael J. Liebowitz C...   \n",
       "19       Roger Edward R. Pauley Edward F. Grant Kelly   \n",
       "\n",
       "                                     application_type        date  \\\n",
       "0                              Waiver of removability  04/30/2013   \n",
       "1   Asylum; withholding of removal; Convention Aga...  08/27/2013   \n",
       "2                                         Termination   9/22/2010   \n",
       "3            Suppression, termination, voluntary depa  03/31/2011   \n",
       "4   Special rule cancellation of removal; cancella...  01/19/2012   \n",
       "5   Asylum; withholding of removal; Convention Aga...  02/04/2014   \n",
       "6   Asylum; withholding of removal; Convention Aga...  04/16/2003   \n",
       "7     Waiver of inadmissibility under section 212(h)   11/18/2013   \n",
       "8   Asylum; withholding of removal; Convention Aga...  06/09/2015   \n",
       "9                                           Reopening  03/21/2016   \n",
       "10  Asylun; withholding of removal; Convention Aga...  03/17/2016   \n",
       "11                                          Reopening  07/05/2016   \n",
       "12                                          Reopening  08/11/2016   \n",
       "13  Asylum; withholding ofremoval; Convention Agai...  07/26/2016   \n",
       "14  Asylum; withholding of removal; Convention Aga...   11/1/2016   \n",
       "15                         Termination of proceedings  12/12/2016   \n",
       "16                                          Reopening  01/06/2017   \n",
       "17  Asylum; withholding of removal; Convention Aga...  10/22/2015   \n",
       "18  Termination; withholding of removal; Conventio...   4/26/2017   \n",
       "19  Cancellation of removal under section 240A(b) ...  05/03/2017   \n",
       "\n",
       "   country_of_origin                         outcome case_origin_state  \\\n",
       "0            Tunisia                       Dismissed                HI   \n",
       "1              Ghana             Sustained; remanded                NJ   \n",
       "2            Unknown             Sustained; remanded                CT   \n",
       "3           Honduras                        Remanded                TN   \n",
       "4             Mexico             Sustained; remanded                CA   \n",
       "5             Mexico             Sustained; remanded                TN   \n",
       "6              Zaire       denied; granted; remanded                MA   \n",
       "7              Haiti             Sustained; remanded                NY   \n",
       "8        El Salvador  dismissed; sustained; remanded                AZ   \n",
       "9             Mexico             Sustained; remanded                TX   \n",
       "10          Honduras             Sustained; remanded                CO   \n",
       "11       El Salvador                        Remanded                CA   \n",
       "12       El Salvador             Sustained; remanded                CA   \n",
       "13        Bangladesh             Sustained; remanded                GA   \n",
       "14       El Salvador                        Remanded                MD   \n",
       "15           Unknown           sustained; terminated                AZ   \n",
       "16   Goiania, Brazil    sustained; remanded; vacated                MA   \n",
       "17         Nicaragua                        Remanded                FL   \n",
       "18             India           sustained; terminated                OH   \n",
       "19            Mexico                        Remanded                CT   \n",
       "\n",
       "   case_origin_city             protected_grounds  \\\n",
       "0          Honolulu                       Unknown   \n",
       "1         Elizabeth             Social, Political   \n",
       "2          Hartford                       Unknown   \n",
       "3           Memphis                       Unknown   \n",
       "4       Los Angeles                       Unknown   \n",
       "5           Memphis                        Social   \n",
       "6            Boston                       Unknown   \n",
       "7          New York                       Unknown   \n",
       "8           Phoenix                        Social   \n",
       "9           Houston                       Unknown   \n",
       "10       Centennial                        Social   \n",
       "11      Los Angeles                       Unknown   \n",
       "12      Los Angeles                       Unknown   \n",
       "13          Lumpkin                     Political   \n",
       "14        Baltimore                        Social   \n",
       "15             Eloy                       Unknown   \n",
       "16          Everett  race, nationality, political   \n",
       "17            Miami                       Unknown   \n",
       "18        Cleveland                       Unknown   \n",
       "19         Hartford                       Unknown   \n",
       "\n",
       "                  type_of_persecution credibility check_for_one_year  \n",
       "0                             Unknown       False              False  \n",
       "1                             Unknown        True               True  \n",
       "2                             Unknown       False              False  \n",
       "3                             Unknown       False              False  \n",
       "4                             Unknown        True              False  \n",
       "5                              Family        True               True  \n",
       "6                             Unknown       False              False  \n",
       "7                             Unknown       False              False  \n",
       "8                              Family       False               True  \n",
       "9                             Unknown       False              False  \n",
       "10                             Family       False              False  \n",
       "11                            Unknown       False              False  \n",
       "12                            Unknown       False              False  \n",
       "13                            Unknown        True              False  \n",
       "14                             Family        True               True  \n",
       "15                            Unknown       False              False  \n",
       "16  Political violence against family       False              False  \n",
       "17                  Domestic Violence       False              False  \n",
       "18                            Unknown       False              False  \n",
       "19                            Unknown       False              False  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>panel_members</th>\n",
       "      <th>application_type</th>\n",
       "      <th>date</th>\n",
       "      <th>country_of_origin</th>\n",
       "      <th>outcome</th>\n",
       "      <th>case_origin_state</th>\n",
       "      <th>case_origin_city</th>\n",
       "      <th>protected_grounds</th>\n",
       "      <th>type_of_persecution</th>\n",
       "      <th>credibility</th>\n",
       "      <th>check_for_one_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140194281-Ali-Fares-A047-654-200-BIA-Apr-30-20...</td>\n",
       "      <td>Michael J. Creppy</td>\n",
       "      <td>Waiver of removability</td>\n",
       "      <td>04/30/2013</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>Dismissed</td>\n",
       "      <td>HI</td>\n",
       "      <td>Honolulu</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165227167-K-O-A-BIA-Aug-27-2013.pdf</td>\n",
       "      <td>Linda S. Wendtland,Roger Pauley,Teresa L. Donovan</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>08/27/2013</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>Social, Political</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171952033-Luis-Narciso-Sedeno-Trujillo-A088-19...</td>\n",
       "      <td>Anne J. Greer,Linda S. Wendtland,Roger Pauley</td>\n",
       "      <td>Termination</td>\n",
       "      <td>9/22/2010</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>CT</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175361890-Jose-Zacaria-Quinteros-A088-239-850-...</td>\n",
       "      <td>Anne J. Greer,Linda S. Wendtland,Roger Pauley</td>\n",
       "      <td>Suppression, termination, voluntary depa</td>\n",
       "      <td>03/31/2011</td>\n",
       "      <td>Honduras</td>\n",
       "      <td>Remanded</td>\n",
       "      <td>TN</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202216334-Francisco-Hernandez-Pina-A073-976-63...</td>\n",
       "      <td>Anne J. Greer,Patricia A. Cole,Roger Pauley</td>\n",
       "      <td>Special rule cancellation of removal; cancella...</td>\n",
       "      <td>01/19/2012</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>205871959-M-G-O-AXXX-XXX-611-BIA-Feb-4-2014.pdf</td>\n",
       "      <td>Linda S. Wendtland,Roger Pauley,Teresa L. Donovan</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>02/04/2014</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>TN</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Social</td>\n",
       "      <td>Family</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>208167027-J-M-S-B-W-AXX-XXX-109-BIA-Apr-4-2003...</td>\n",
       "      <td>Edward R. Grant</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>04/16/2003</td>\n",
       "      <td>Zaire</td>\n",
       "      <td>denied; granted; remanded</td>\n",
       "      <td>MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>225423441-Roberson-Joseph-A078-360-606-BIA-Nov...</td>\n",
       "      <td>Anne J. Greer,Linda S. Wendtland,Roger Pauley</td>\n",
       "      <td>Waiver of inadmissibility under section 212(h)</td>\n",
       "      <td>11/18/2013</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>271354416-D-M-R-BIA-June-9-2015.pdf</td>\n",
       "      <td>Anne J. Greer,Charles K. Adkins-Blanch,Neil P....</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>06/09/2015</td>\n",
       "      <td>El Salvador</td>\n",
       "      <td>dismissed; sustained; remanded</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Social</td>\n",
       "      <td>Family</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>309306110-Sergio-Maldonado-A078-567-541-BIA-Ma...</td>\n",
       "      <td>Brian M. O'Leary,Edward R. Grant,John Guendels...</td>\n",
       "      <td>Reopening</td>\n",
       "      <td>03/21/2016</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>311736830-H-R-M-AXXX-XXX-381-BIA-March-14-2016...</td>\n",
       "      <td>Anne J. Greer,Charles K. Adkins-Blanch,Margare...</td>\n",
       "      <td>Asylun; withholding of removal; Convention Aga...</td>\n",
       "      <td>03/17/2016</td>\n",
       "      <td>Honduras</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>CO</td>\n",
       "      <td>Centennial</td>\n",
       "      <td>Social</td>\n",
       "      <td>Family</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>318845018-Ennio-Edgardo-Galvez-A095-006-115-BI...</td>\n",
       "      <td>Edward R. Grant</td>\n",
       "      <td>Reopening</td>\n",
       "      <td>07/05/2016</td>\n",
       "      <td>El Salvador</td>\n",
       "      <td>Remanded</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>322069426-Maricela-Jacquelin-Madrid-Gomez-A206...</td>\n",
       "      <td>Blair O'Connor,Brian M. O'Leary,Charles K. Adk...</td>\n",
       "      <td>Reopening</td>\n",
       "      <td>08/11/2016</td>\n",
       "      <td>El Salvador</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>328162186-H-B-AXXX-XXX-608-BIA-July-26-2016.pdf</td>\n",
       "      <td>Anne J. Greer,Linda S. Wendtland,Roger Pauley</td>\n",
       "      <td>Asylum; withholding ofremoval; Convention Agai...</td>\n",
       "      <td>07/26/2016</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Sustained; remanded</td>\n",
       "      <td>GA</td>\n",
       "      <td>Lumpkin</td>\n",
       "      <td>Political</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>334139459-S-V-C-AXXX-XXX-431-BIA-Nov-1-2016.pdf</td>\n",
       "      <td>Anrie J. Greer,Charles K. Adkins-Blanch,Margar...</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>11/1/2016</td>\n",
       "      <td>El Salvador</td>\n",
       "      <td>Remanded</td>\n",
       "      <td>MD</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>Social</td>\n",
       "      <td>Family</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>337539141-Bertha-A-Lopez-Tovar-A071-904-519-BI...</td>\n",
       "      <td>Joan B Geller,John Guendelsberger,Roger Pauley</td>\n",
       "      <td>Termination of proceedings</td>\n",
       "      <td>12/12/2016</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>sustained; terminated</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Eloy</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>337540716-Clebson-Sousa-Carneiro-A078-254-701-...</td>\n",
       "      <td>Blair Ana O'Connor Mann Charles K. Adkins-Blanch</td>\n",
       "      <td>Reopening</td>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>Goiania, Brazil</td>\n",
       "      <td>sustained; remanded; vacated</td>\n",
       "      <td>MA</td>\n",
       "      <td>Everett</td>\n",
       "      <td>race, nationality, political</td>\n",
       "      <td>Political violence against family</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>337542906-A-A-L-M-AXXX-XXX-585-BIA-Oct-22-2015...</td>\n",
       "      <td>Margaret M Neil P. O'Herron Anne J. Miller Greer</td>\n",
       "      <td>Asylum; withholding of removal; Convention Aga...</td>\n",
       "      <td>10/22/2015</td>\n",
       "      <td>Nicaragua</td>\n",
       "      <td>Remanded</td>\n",
       "      <td>FL</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Domestic Violence</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>349320269-S-D-AXXX-XXX-230-BIA-April-26-2017.pdf</td>\n",
       "      <td>Hugh G. Ellen C Mullane Michael J. Liebowitz C...</td>\n",
       "      <td>Termination; withholding of removal; Conventio...</td>\n",
       "      <td>4/26/2017</td>\n",
       "      <td>India</td>\n",
       "      <td>sustained; terminated</td>\n",
       "      <td>OH</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>349321596-Adela-Nolasco-Santiago-A205-497-497-...</td>\n",
       "      <td>Roger Edward R. Pauley Edward F. Grant Kelly</td>\n",
       "      <td>Cancellation of removal under section 240A(b) ...</td>\n",
       "      <td>05/03/2017</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Remanded</td>\n",
       "      <td>CT</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "df_scrape.to_csv('df_scrape.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "df_manual.to_csv('df_manual.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "from LambdaLib.Analysis.csv_similarity import csv_similarity_score\n",
    "\n",
    "score = csv_similarity_score(\"df_scrape.csv\", \"df_manual.csv\")\n",
    "print(score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6260795560558698\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('human-rights-first-asylum-ds-a-bUMUEGIo': pipenv)"
  },
  "interpreter": {
   "hash": "f9a00ff7b4804186bed77c4f59995e5e4a690b80622b30d4fb44d07178b0cbe5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}