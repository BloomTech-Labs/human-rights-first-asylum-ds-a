{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FibDZlR2iQD8"
   },
   "source": [
    "# BOARD OF IMMIGRATION APPEALS DOCUMENT SCRAPER\n",
    "\n",
    "• This scraper uses the spaCy library in order to gather information about the case. There is no way to ensure it is 100% accurate, except to scrape a document and compare the information gathered directly with the original PDF. \n",
    "\n",
    "• This code currently lives inside a Jupyter Notebook for ease of testing and iteration, but will ultimately need to graduate into a standard .py file. \n",
    "\n",
    "• Scraping is extremely difficult task, as in most cases just searching the document for a keyword isn't enough. It must live in the correct context. No douct the code in this notebook can constantly be improved for a very long time. Before delving into this code, I recommend reading a good chunk of case file PDFs in order to get aquianted with their structre and information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS3b78vODefG"
   },
   "source": [
    "#### IMPORTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jEsm4q0_S7mn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy\n",
    "!pip install bs4\n",
    "!pip install geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m21I3dxQSwhd",
    "outputId": "6240ac11-8af2-4024-adba-5688634b9b23"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geonamescache'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b7a1c0242c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeonamescache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geonamescache'"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union, Callable, Dict, Iterator\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup, element\n",
    "import geonamescache\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens.span import Span\n",
    "from spacy.tokens.token import Token\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "\n",
    "# I currently interface with the text files through my google drive.\n",
    "# The relevant documents should be able upon request through Slack,\n",
    "# and ideally should at some point be stored and obtained through a DB\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Dy3MWnDjEm"
   },
   "source": [
    "#### FILES ARE MOUNTED FROM GOOGLE DRIVE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c2cTaSDPb9yK"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6dcaf74cf54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mASYLUM_DIR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mASYLUM_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./drive/MyDrive/scraped_asylum_cases\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mASYLUM_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "ASYLUM_DIR: Path\n",
    "ASYLUM_DIR = Path(\"./drive/MyDrive/scraped_asylum_cases\")\n",
    "\n",
    "text_files: List[str]\n",
    "text_files = os.listdir(ASYLUM_DIR)\n",
    "\n",
    "def get_text_from(text_file: str, dir: str = ASYLUM_DIR):\n",
    "    with open(dir / text_file) as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "ex_text = get_text_from(text_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a6vRaYDB0Mib"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0482fca98f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading in the spaCy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# loading in the spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5MsPRGADDVH"
   },
   "source": [
    "#### GATHER BIA JUDGES:\n",
    "• The judges that compromise the Board of Immigration Appeals were scraped off of the BIA's wikipedia, which will only show the current judges. This means that in any older PDF files, a judge that is no longer in the BIA is unaccounted for. I could not find a list of previous BIA judges, and focused on these only, for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "Ian7APwuirJU",
    "outputId": "9882d3e8-9e18-4ccd-e430-2e9f9b432d03"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5663e8256955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudges_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting current Appellate Immigration Judges\n",
    "# From Wikipedia, using Beautiful Soup. Code is mostly biolerplate\n",
    "judges_url: str\n",
    "judges_url = 'https://en.wikipedia.org/wiki/Board_of_Immigration_Appeals'\n",
    "\n",
    "html: str\n",
    "html = requests.get(judges_url).text\n",
    "\n",
    "soup: BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "table: element.Tag\n",
    "table = soup.findAll(lambda tag: tag.name =='table')[1]\n",
    "\n",
    "rows: element.ResultSet\n",
    "rows = table.findAll(lambda tag: tag.name == 'tr')\n",
    "\n",
    "column_names: List[str]\n",
    "column_names = [\n",
    "    col.get_text().strip().lower().replace(' ', '_')\n",
    "    for col in rows.pop(0) if col.name == 'th'\n",
    "]\n",
    "\n",
    "rows: List[List[str]]\n",
    "rows = [\n",
    "    [\n",
    "        cell.get_text().strip().replace(',', '') \n",
    "        for cell in row if cell.name == 'td'\n",
    "    ] \n",
    "    for row in rows\n",
    "]\n",
    "\n",
    "as_dict: List[Dict[str, str]]\n",
    "as_dict = [\n",
    "    dict(zip(column_names, row)) for row in rows\n",
    "]\n",
    "\n",
    "judges_df: pd.DataFrame\n",
    "judges_df = pd.DataFrame.from_dict(as_dict)\n",
    "\n",
    "judges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeN1jtnRkBgs"
   },
   "source": [
    "#### UTILITIES:\n",
    "\n",
    "• The Optical Character Recognition (OCR) used through pytesseract to convert PDFs to text ISN'T perfect. Often artifacts show up, such as the character 'l' shown up as '!', or 'nn' as 'm'. \n",
    "\n",
    "• The difflib library comes in handy here in order to compare strings by inexact values. The more two strings are alike, the sequence matcher ratio will approach 1, the less they are alike, it will approach 0.\n",
    "\n",
    "• I recently found a library that could offer a better implementation. It's called FuzzyWuzzy, future teams should look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bepD85LnBm9x"
   },
   "outputs": [],
   "source": [
    "# I would like to later reimplement these with the python\n",
    "# library FuzzyWuzzy, look it up!\n",
    "def similar(a: str, return_b: str, min_score: float) -> str:\n",
    "    '''\n",
    "    • Returns 2nd string if similarity score is above supplied\n",
    "    minimum score. Else, returns None.\n",
    "    '''\n",
    "    return return_b \\\n",
    "        if SequenceMatcher(None, a, return_b).ratio() >= min_score \\\n",
    "        else None\n",
    "\n",
    "# this function implements the similar function, but on a list\n",
    "# it uses a closure in order to return a function\n",
    "def similar_in_list(lst: List[str]) -> Callable:\n",
    "    '''\n",
    "    • Uses a closure on supplied list to return a function that iterates over\n",
    "    the list in order to search for the first similar term. It's used widely\n",
    "    in the scraper.\n",
    "    '''\n",
    "    def impl(item: str, min_score: float) -> Union[str, None]:\n",
    "        for s in lst:\n",
    "            s = similar(item, s, min_score)\n",
    "            if s:\n",
    "                return s\n",
    "        return None\n",
    "    return impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzJKypoKCleI"
   },
   "source": [
    "• In the judge name matching algorithm below, I made a last minute change. Originally is was checking very strictly (First, Last, and any intials must match), but for example, I realized it wasn't picking up the judge Cassidy. I changed the algorithm just to check if the last name is present, which resolved the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "m052dd5O_Dvi"
   },
   "outputs": [],
   "source": [
    "def get_if_judge(name: str) -> Union[str, None]:\n",
    "    '''\n",
    "    • Returns the judge's name if a match is found. Currently, the match\n",
    "    is very strictly defined by the current judge's names found through\n",
    "    Wikipedia. It will 100% stop any false positives, but some leniency should\n",
    "    be introduced in order to prevent any false negatives.\n",
    "    '''\n",
    "\n",
    "    clean_name: Callable[[str], str]\n",
    "    clean_name = lambda s: s.lower() \\\n",
    "                            .replace(',', '') \\\n",
    "                            .replace('.', '')\n",
    "\n",
    "    judges_names = judges_df['name'] \\\n",
    "                    .apply(lambda s: clean_name(s).split()[-1])\n",
    "\n",
    "    name = clean_name(name).split()[-1]\n",
    "\n",
    "    for i, jn in enumerate(judges_names):\n",
    "        if jn in name:\n",
    "            return judges_df['name'].iloc[i]\n",
    "    \n",
    "    return None\n",
    "\n",
    "    # # Tuple of split, sorted judge name, and original judge name\n",
    "    # judges_names: List[Tuple[List[str], str]]\n",
    "    # judges_names = [\n",
    "    #     (sorted(clean_name(jn).split()), jn)\n",
    "    #     for jn in judges_df['name']\n",
    "    # ]   \n",
    "    \n",
    "    # name: List[str]\n",
    "    # name = sorted(map(clean_name, name.split()))\n",
    "\n",
    "    # for jn_low, jn in judges_names:\n",
    "    #     is_judge: bool\n",
    "    #     is_judge = all([\n",
    "    #         similar(n, j, 0.8)\n",
    "    #         for n, j in zip(name, jn_low)\n",
    "    #         if len(n) != 1\n",
    "    #     ])\n",
    "\n",
    "    #     if is_judge:\n",
    "    #         return jn\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wG0fYt4vGgxh"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'geonamescache' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-63b34078cf7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use geonamescache library in order to get current list of all countries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeonamescache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeonamesCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCOUNTRIES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCOUNTRIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_countries_by_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'geonamescache' is not defined"
     ]
    }
   ],
   "source": [
    "# Use geonamescache library in order to get current list of all countries\n",
    "gc = geonamescache.GeonamesCache()\n",
    "\n",
    "COUNTRIES: Iterator[str]\n",
    "COUNTRIES = gc.get_countries_by_names().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnakkL3bEEWq"
   },
   "source": [
    "#### BIA SCRAPER CLASS:\n",
    "• A smarter implementation of this in a notebook would have been separating these methods out into individual functions in different cells. I preemtively set it up this way imagining it would improve optimization, but with the small size of the documents we are handling, optimization should not be a huge concern. Experimentation and validation should be the #1 priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AXvG3fFeJaz"
   },
   "outputs": [],
   "source": [
    "class BIACase:\n",
    "    def __init__(self, text: str):\n",
    "        '''\n",
    "        • Input will be text from a BIA case pdf file, after the pdf has\n",
    "        been converted from PDF to text. \n",
    "        • Scraping works utilizing spaCy, tokenizing the text, and iterating\n",
    "        token by token searching for matching keywords.\n",
    "        '''\n",
    "\n",
    "        self.doc: Doc \n",
    "        self.doc = nlp(text)\n",
    "\n",
    "        self.ents: Tuple[Span] \n",
    "        self.ents = self.doc.ents\n",
    "\n",
    "    def get_ents(self, labels: List[str] = None) -> Iterator[Span]:\n",
    "        '''\n",
    "        • Retrieves entitiess of a specified label(s) in the document,\n",
    "        if no label is specified, returns all entities\n",
    "        '''\n",
    "        return (ent for ent in self.ents if ent.label_ in labels) \\\n",
    "                if labels \\\n",
    "                else self.ents\n",
    "\n",
    "    def get_country_of_origin(self) -> str:\n",
    "        '''\n",
    "        • Returns the country of origin of the applicant. Currently just checks\n",
    "        the document for a country that is NOT the United States.\n",
    "        '''\n",
    "        locations: Iterator[str]\n",
    "        locations = map(lambda ent: ent.text,\n",
    "                        self.get_ents(['GPE']))\n",
    "        \n",
    "        similar_country: Callable[[str, float], Union[str, None]]\n",
    "        similar_country = similar_in_list(COUNTRIES)\n",
    "\n",
    "        for loc in locations:\n",
    "            if not similar(loc, 'United States', 0.9):\n",
    "                origin: Union[str, None]\n",
    "                origin = similar_country(loc, 0.9)\n",
    "\n",
    "                if origin: return origin\n",
    "                else: continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "    def get_date(self) -> Union[str, None]:\n",
    "        '''\n",
    "        • Returns date of the document. Easy to validate by the PDF filename,\n",
    "        whether its hosted on scribd or somewhere else.\n",
    "        '''\n",
    "        clean_date: Callable[[str], str]\n",
    "        clean_date = lambda s: ''.join([\n",
    "            char for char in s\n",
    "                if char.isalnum()\n",
    "                or char.isspace()\n",
    "        ])\n",
    "\n",
    "        dates: Iterator[str] \n",
    "        dates = map(lambda ent: clean_date(ent.text), \n",
    "                    self.get_ents(['DATE']))\n",
    "\n",
    "        for date in dates:\n",
    "            try:\n",
    "                # SHOULD return list of length 3,\n",
    "                # Such as ['Sept', '2', '2019']\n",
    "                d: List[str]\n",
    "                d = date.split()\n",
    "                if len(d) != 3:\n",
    "                    continue\n",
    "                else:\n",
    "                    # Ex. Jan, Feb, ..., Sep, Oct, Dec\n",
    "                    month: str\n",
    "                    month = d[0][:3].title()\n",
    "                    # Ex. 01, 02, 03, ..., 29, 30, 31\n",
    "                    day: str\n",
    "                    day = '0' + d[1] \\\n",
    "                        if len(d[1]) == 1 else d[1]\n",
    "                    # Ex. 1991, 1992, ..., 2020, 2021\n",
    "                    year: str\n",
    "                    year = d[2]\n",
    "                    # Ex. Jan 09 2014\n",
    "                    parsed_date: str\n",
    "                    parsed_date = ' '.join([month, day, year])\n",
    "                    # datetime obj, Ex Repr: 2020-09-24 00:00:00\n",
    "                    dt: datetime\n",
    "                    dt = datetime.strptime(parsed_date, '%b %d %Y')\n",
    "                    # strip time of hours/min/sec, save as str\n",
    "                    dt: str\n",
    "                    dt = str(dt).split()[0]\n",
    "                    \n",
    "                    return dt\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_panel(self) -> Union[List[str], None]:\n",
    "        '''\n",
    "        • Returns the panel members of case in document. \n",
    "        TODO: Check judges names less strictly - I've seen a document\n",
    "        that named the Judge Monsky differently than how she regularly \n",
    "        appears.\n",
    "        '''\n",
    "        panel_members = List[str]\n",
    "        panel_members = []\n",
    "\n",
    "        possible_members: Iterator[Span]\n",
    "        possible_members = map(lambda ent: ent.text,\n",
    "                               self.get_ents(['PERSON', 'ORG']))\n",
    "\n",
    "        for member in possible_members:\n",
    "            judge: Union[str, None]\n",
    "            judge = get_if_judge(member)\n",
    "\n",
    "            if judge:\n",
    "                panel_members.append(judge)\n",
    "\n",
    "        return list(set(panel_members)) \\\n",
    "               if panel_members \\\n",
    "               else None\n",
    "\n",
    "    def get_surrounding_sents(self, token: Token) -> Span:\n",
    "        '''\n",
    "        • This function will return the two sentences surrounding the token,\n",
    "        including the sentence holding the token.\n",
    "        '''\n",
    "        start: int\n",
    "        start = token.sent.start\n",
    "\n",
    "        end: int\n",
    "        end = token.sent.end\n",
    "\n",
    "        try:\n",
    "            sent_before_start: int\n",
    "            sent_before_start = self.doc[start-1].sent.start\n",
    "            sent_after_end: int\n",
    "            sent_after_end = self.doc[end+1].sent.end\n",
    "        except:\n",
    "            return token.sent\n",
    "\n",
    "        surrounding: Span\n",
    "        surrounding = self.doc[sent_before_start:sent_after_end+1]\n",
    "\n",
    "        return surrounding\n",
    "\n",
    "    def get_protected_grounds(self) -> Union[List[str], None]:\n",
    "        '''\n",
    "        • This will return the protected ground(s) of the applicant. Special\n",
    "        checks are needed. Checking for keywords is not enough, as sometimes\n",
    "        documents label laws that describe each protected ground. Examples\n",
    "        are 'Purely Political Offense' and 'Real Id Act'.\n",
    "        '''\n",
    "        protected_grounds: List[str]\n",
    "        protected_grounds = [\n",
    "            'race',\n",
    "            'religion',\n",
    "            'nationality',\n",
    "            'social',\n",
    "            'political',\n",
    "        ]\n",
    "\n",
    "        pgs = []\n",
    "\n",
    "        similar_pg: Callable[[str, float], Union[str, None]]\n",
    "        similar_pg = similar_in_list(protected_grounds)\n",
    "\n",
    "        for token in self.doc:\n",
    "\n",
    "            sent: str\n",
    "            sent = token.sent.text.lower()\n",
    "\n",
    "            s: Union[str, None]\n",
    "            s = similar_pg(token.text.lower(), 0.9)\n",
    "\n",
    "            if s == 'social':\n",
    "                next_word = self.doc[token.i+1].text.lower()\n",
    "                if not similar(next_word, 'group', 0.95): \n",
    "                    continue\n",
    "\n",
    "            elif s == 'political':\n",
    "                next_word = self.doc[token.i+1].text.lower()\n",
    "                if similar(next_word, 'offense', 0.95): \n",
    "                    continue\n",
    "\n",
    "            elif s == 'nationality':\n",
    "                next_word = self.doc[token.i+1].text.lower()\n",
    "                if similar(next_word, 'act', 1):\n",
    "                    continue\n",
    "\n",
    "            if s:\n",
    "                surrounding: Span\n",
    "                surrounding = self.get_surrounding_sents(token)\n",
    "                \n",
    "                if 'real id' in sent:\n",
    "                    continue\n",
    "                elif 'grounds specified' in surrounding.text.lower():\n",
    "                    continue \n",
    "                elif 'no claim' in surrounding.text.lower():\n",
    "                    continue\n",
    "\n",
    "                pgs.append(s)\n",
    "\n",
    "        return list(set(pgs)) if pgs else None\n",
    "\n",
    "    def get_application(self) -> Dict[str, bool]:\n",
    "        '''\n",
    "        • This will return the seeker's application, found after 'APPLICATION'.\n",
    "        Because HRF is only interested in Asylum, Withholding of Removal,\n",
    "        and Convention Against Torture applications, the others should be\n",
    "        ignored and not included in the dataset.\n",
    "        '''\n",
    "\n",
    "        relevant_applications: List[str]\n",
    "        relevant_applications = [\n",
    "            'asylum',\n",
    "            'withholding',\n",
    "            'torture'    \n",
    "        ]\n",
    "\n",
    "        similar_app: Callable[[str, float], Union[str, None]]\n",
    "        similar_app = similar_in_list(relevant_applications)\n",
    "\n",
    "        app: Dict[str, bool]\n",
    "        application = {\n",
    "            'asylum': False,\n",
    "            'withholding_of_removal': False,\n",
    "            'CAT': False\n",
    "        }\n",
    "\n",
    "        for token in self.doc:\n",
    "            if similar(token.text, 'APPLICATION', .86):\n",
    "                for i in range(1,30):\n",
    "                    word: str\n",
    "                    word = self.doc[i + token.i].text.lower()\n",
    "\n",
    "                    app: Union[str, None]\n",
    "                    app = similar_app(word, 0.9)\n",
    "\n",
    "                    if app == 'asylum':\n",
    "                        application['asylum'] = True\n",
    "                    elif app == 'withholding':\n",
    "                        application['withholding_of_removal'] = True\n",
    "                    elif app == 'torture':\n",
    "                        application['CAT'] = True\n",
    "\n",
    "        return application\n",
    "\n",
    "    def get_outcome(self) -> Union[str, None]:\n",
    "        '''\n",
    "        • Returns the outcome of the case. This will appear after 'ORDER'\n",
    "        at the end of the document.\n",
    "        '''\n",
    "        outcomes: List[str]\n",
    "        outcomes = [\n",
    "            'remanded', \n",
    "            'reversal', \n",
    "            'dismissed', \n",
    "            'sustained', \n",
    "            'terminated', \n",
    "            'granted', \n",
    "            'denied', \n",
    "            'returned'\n",
    "        ]\n",
    "\n",
    "        outcomes: Iterator[str]\n",
    "        outcomes_lemma = map(lambda s: nlp(s)[0].lemma_, outcomes)\n",
    "\n",
    "        similar_outcome: Callable[[str, float], Union[str, None]]\n",
    "        similar_outcome = similar_in_list(outcomes)\n",
    "\n",
    "        similar_outcome_l: Callable[[str, float], Union[str, None]]\n",
    "        similar_outcome_l = similar_in_list(outcomes)\n",
    "\n",
    "        dlen: int\n",
    "        dlen = len(self.doc)\n",
    "\n",
    "        # iterating token by token through document in reverse\n",
    "        # improves efficiency only slightly\n",
    "        for i in reversed(range(dlen-1)):\n",
    "            token: Token\n",
    "            token = self.doc[i]\n",
    "\n",
    "            if similar(token.text, 'ORDER', 0.9):\n",
    "                for ii in range(i+1, dlen):\n",
    "                    o: Union[str, None]\n",
    "                    o = similar_outcome(self.doc[ii].text, 0.9)\n",
    "                    o = o if o else similar_outcome_l(self.doc[ii].text, 0.92)\n",
    "                    if o:\n",
    "                        return nlp(o)[0].lemma_\n",
    "        return None\n",
    "\n",
    "    def get_based_violence(self) -> Union[Dict[str, List[str]], None]:\n",
    "        '''\n",
    "        • Returns a dictionary where the keys are:\n",
    "            Family-based violence,\n",
    "            Gender-based violence,\n",
    "            Gang-based violence\n",
    "        • If a key is in the dict, it means the based_violence is present\n",
    "        in the document, and the relevant sentence(s) where the information is\n",
    "        contained in the key's value\n",
    "        '''\n",
    "        violent_terms: List[str]\n",
    "        violent_terms = [\n",
    "            'hurt',\n",
    "            'kill',\n",
    "            'rape',\n",
    "            'assassinate',\n",
    "            'abuse',\n",
    "            'threat',\n",
    "            'murder',\n",
    "            'torture',\n",
    "            'assault',\n",
    "            'shoot',\n",
    "            'suffer',\n",
    "            'abduct',\n",
    "            'kidnap',\n",
    "            'harm',\n",
    "            'persecute',\n",
    "            'scare',\n",
    "            'fear'\n",
    "        ]\n",
    "\n",
    "        sg_family: List[str]\n",
    "        sg_family = [\n",
    "            'family',\n",
    "            'woman',\n",
    "            'partner',\n",
    "            'husband',\n",
    "            'wife',\n",
    "            'son',\n",
    "            'daughter',\n",
    "            'child',\n",
    "            'ethnicity',\n",
    "            'parent'\n",
    "        ]\n",
    "\n",
    "        sg_gender: List[str]\n",
    "        sg_gender = [\n",
    "            'sex'\n",
    "            'gender',\n",
    "            'sexuality',\n",
    "            'woman',\n",
    "            'transgender',\n",
    "            'lgbt',\n",
    "            'lgbtq',\n",
    "            'lgbtqia',\n",
    "            'homosexual',\n",
    "            'homosexuality',\n",
    "            'gay',\n",
    "            'lesbian',\n",
    "            'queer',\n",
    "        ]\n",
    "\n",
    "        similar_vterm: Callable[[str, float], Union[str, None]]\n",
    "        similar_vterm = similar_in_list(violent_terms)\n",
    "\n",
    "        similar_sg_family: Callable[[str, float], Union[str, None]]\n",
    "        similar_sg_family = similar_in_list(sg_family)\n",
    "\n",
    "        similar_sg_gender: Callable[[str, float], Union[str, None]]\n",
    "        similar_sg_gender = similar_in_list(sg_gender)\n",
    "\n",
    "        based_v = defaultdict(lambda: [])\n",
    "\n",
    "        for token in self.doc:\n",
    "            if similar_sg_family(token.lemma_.lower(), 0.9):\n",
    "                sent: Span\n",
    "                sent = token.sent\n",
    "                for w in sent:\n",
    "                    vterm = similar_vterm(w.lemma_.lower(), 0.86)\n",
    "                    if vterm and 'statute' not in token.sent.text:\n",
    "                        based_v['family-based'] += [token.lemma_.lower()]\n",
    "\n",
    "            elif similar_sg_gender(token.text.lower(), 0.86):\n",
    "                sent: Span\n",
    "                sent = self.get_surrounding_sents(token)\n",
    "                for w in sent:\n",
    "                    vterm = similar_vterm(w.lemma_.lower(), 0.86)\n",
    "                    if vterm and 'statute' not in token.sent.text:\n",
    "                        based_v['gender-based'] += [token.lemma_.lower()]\n",
    "            \n",
    "            elif similar(token.text.lower(), 'gang', 0.9):\n",
    "                sent = token.sent\n",
    "                based_v['gang-based'] += [token.lemma_.lower()]\n",
    "\n",
    "        if based_v:\n",
    "            based_v: Dict[str, List[str]]\n",
    "            based_v = {k:list(set(v)) for k, v in based_v.items()}\n",
    "\n",
    "        return based_v if based_v else None\n",
    "\n",
    "    def references_AB27_216(self) -> bool:\n",
    "        '''\n",
    "        • Returns True if the case file mentions \n",
    "        Matter of AB, 27 I&N Dec. 316 (A.G. 2018)\n",
    "        '''\n",
    "        for token in self.doc:\n",
    "            if token.text == 'I&N':\n",
    "                sent = token.sent.text\n",
    "                if '316' in sent and '27' in sent:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def references_LEA27_581(self) -> bool:\n",
    "        '''\n",
    "        • Returns True if the case file mentions \n",
    "        Matter of L-E-A-, 27 I&N Dec. 581 (A.G. 2019)\n",
    "        '''\n",
    "        for sent in self.doc.sents:\n",
    "            if 'L-E-A-' in sent.text:\n",
    "                if '27' in sent.text:\n",
    "                    return True\n",
    "        return False\n",
    "                \n",
    "    def get_seeker_sex(self) -> str:\n",
    "        '''\n",
    "        • This field needs to be validated. Currently, it assumes the \n",
    "        sex of the seeker by the number of instances of pronouns in the \n",
    "        document.\n",
    "        '''\n",
    "        male: int\n",
    "        male = 0\n",
    "\n",
    "        female: int\n",
    "        female = 0\n",
    "\n",
    "        for token in self.doc:\n",
    "            if similar(token.text, 'he', 1) \\\n",
    "                or similar(token.text, 'him', 1) \\\n",
    "                or similar(token.text, 'his', 1):\n",
    "                male += 1\n",
    "            elif similar(token.text, 'she', 1) \\\n",
    "                or similar(token.text, 'her', 1):\n",
    "                female += 1\n",
    "\n",
    "        return 'male' if male > female \\\n",
    "                else 'female' if female > male \\\n",
    "                else 'unkown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtNqYRO0EyI8"
   },
   "source": [
    "#### ASSEMBLING DATA:\n",
    "• I assembled data here in order to fit the structure of a csv, but any implementation could have been made. CSV has the pro of being easily manipulated by the Pandas library, but web people particularly like their JSON. The only downside to CSV is that most of these fields are categorical, and can contain 0 to n \"tags\" that would be better represented in a DB schema. You can see specifically how I manipulated this CSV in the streamlit repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN87zvLWi1rB"
   },
   "outputs": [],
   "source": [
    "text_files = os.listdir(ASYLUM_DIR)\n",
    "data = []\n",
    "\n",
    "for i, f in enumerate(text_files):\n",
    "\n",
    "    case = BIACase(get_text_from(f))\n",
    "    case_data = {}\n",
    "\n",
    "    remove_pw: Callable[[str], str]\n",
    "    remove_pw = lambda s: s[:s.find('?secret')]\n",
    "\n",
    "    case_data['filename'] = remove_pw(f) if 'password' in f else f[:-4]\n",
    "\n",
    "    app = case.get_application()\n",
    "    app = [ap for ap, b in app.items() if b]\n",
    "    case_data['application'] = '; '.join(app) if app else None\n",
    "\n",
    "    case_data['date'] = case.get_date()\n",
    "\n",
    "    case_data['country_of_origin'] = case.get_country_of_origin()\n",
    "\n",
    "    panel = case.get_panel()\n",
    "    case_data['panel_members'] = '; '.join(panel) if panel else None\n",
    "\n",
    "    case_data['outcome'] = case.get_outcome()\n",
    "\n",
    "    pgs = case.get_protected_grounds()\n",
    "    case_data['protected_grounds'] = '; '.join(pgs) if pgs else None\n",
    "\n",
    "    based_violence = case.get_based_violence()\n",
    "    violence = '; '.join([k for k, v in based_violence.items() if v]) \\\n",
    "                    if based_violence \\\n",
    "                    else None\n",
    "\n",
    "    keywords = '; '.join(['; '.join(v) for v in based_violence.values()]) \\\n",
    "                    if based_violence \\\n",
    "                    else None\n",
    "            \n",
    "    case_data['based_violence'] = violence\n",
    "    case_data['keywords'] = keywords\n",
    "\n",
    "    references = [\n",
    "        'Matter of AB, 27 I&N Dec. 316 (A.G. 2018)' \n",
    "        if case.references_AB27_216() else None,\n",
    "        'Matter of L-E-A-, 27 I&N Dec. 581 (A.G. 2019)'\n",
    "        if case.references_LEA27_581() else None\n",
    "    ]\n",
    "    \n",
    "    case_data['references'] = '; '.join([r for r in references if r])\n",
    "    case_data['sex_of_applicant'] = case.get_seeker_sex()\n",
    "\n",
    "    data.append(case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oWg3WeMoOel"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df = df.fillna(value=np.nan)\n",
    "\n",
    "df.to_csv('asylum_cases.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dccBfiXlpbx",
    "outputId": "a5021e12-811c-4551-c4b2-99aa81eb8193"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-52a468cfd113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spaCy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bs4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geonamescache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeonamescache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "print('spaCy',spacy.__version__)\n",
    "print('bs4', bs4.__version__)\n",
    "print('geonamescache', geonamescache.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fz8QG-CFyWZ"
   },
   "source": [
    "# CLOSING NOTES:\n",
    "\n",
    "• There are numerous ways text mining can be done. These documents are fairly structured and consistent generally, but there are many 'little things' that need to be accounted for and can only be discovered through the process of getting aquanted with the documents and experimenting with the scraper. Learning the domain is the hardest part, we're not lawyers.\n",
    "\n",
    "• Something that I avoided doing was relying on newline breaks that typically structure these documents. That's 100% an alternative to explore, and the text that gets converted from PDFs generally preverse these newline separations.\n",
    "\n",
    "• Finally, and good framework to apply when gathering data from these documents is accounting for false negatives and false positives and how you can minimize the two. Again, the only way to validate it to test a batch of documents, see if the data acquired is accurate to what the acutal document says, and go from there. It can be a slow and tedious process somtimes, but it's a fascinating one if you're interested in practicing/learning NLP and text mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BIA_SCRAPER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}