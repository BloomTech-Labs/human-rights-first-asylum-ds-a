{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled33.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Vk7RhtorXt",
        "outputId": "264eafe4-b042-4d41-b452-3b0b27cb6fd8"
      },
      "source": [
        "#These are the libraries and packages you will need to install to run this colab\n",
        "!pip install -U geonamescache\n",
        "!pip install -U spacy==3.0.6\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: geonamescache in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already up-to-date: spacy==3.0.6 in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (8.0.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (1.7.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy==3.0.6) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0.6) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.6) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0.6) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.0.6) (3.0.0)\n",
            "2021-05-28 21:56:45.940280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-md==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl#egg=en_core_web_md==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (56.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (1.15.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i_MB-O9nc-I"
      },
      "source": [
        "#set up the nlp, we use md for semantic analysis\n",
        "# nlp = load(\"en_core_web_md\")\n",
        "\n",
        "#If the above code does not work use this!\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbK5wFz1IW4Q"
      },
      "source": [
        "#When using this colab document you want to save the folder hrfCases on your drive. I have it under a Lambda subfolder that is saved as path. I also set the court_locations.json as a google drive file and point to it.\n",
        "import pandas as pd\n",
        "import os, sys, json\n",
        "path = '/content/drive/MyDrive/Lambda/hrfCases'\n",
        "filenames = os.listdir(path)\n",
        "with open('/content/court_locations.json') as x:\n",
        "  court_locs = json.load(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aELfhm5XhDQs"
      },
      "source": [
        "\"\"\"\n",
        "IMPORTS/LIBS\n",
        "Imports of libraries and packages, including spacy nlp library, and reading in\n",
        "court location dictionary\n",
        "\"\"\"\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import time\n",
        "import geonamescache\n",
        "import re\n",
        "import requests\n",
        "import pytesseract\n",
        "from collections import Counter, defaultdict\n",
        "from pdf2image import convert_from_bytes\n",
        "from spacy import load\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "from spacy.matcher import Matcher, PhraseMatcher\n",
        "from typing import List, Tuple, Union, Callable, Dict, Iterator\n",
        "\n",
        "\n",
        "# Read in dictionary of all court locations\n",
        "with open('/content/court_locations.json') as x:\n",
        "    court_locs = json.load(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "SIMILAR/Matcher functions\n",
        "functions that search court document texts for phrases or specific words; used\n",
        "in get_outcome, get_country_of_origin, get_outcome\n",
        "\"\"\"\n",
        "\n",
        "def similar(self, matcher_pattern):\n",
        "    \"\"\"\n",
        "    A function that uses a spacy Matcher object to search for words or\n",
        "    consecutive words as a phrase.\n",
        "\n",
        "    Format: pattern = [{\"LOWER\": <word>}, {\"LOWER\": <the next word>}, ...etc]\n",
        "    Can look for multiple patterns simultaneously using list of patterns;\n",
        "        [[{\"ARG\": word}], [{\"ARG\": word}], [{\"ARG\": word}]]\n",
        "\n",
        "    DOC: https://spacy.io/usage/rule-based-matching\n",
        "    \"\"\"\n",
        "    # create matcher object\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    # Add the pattern that will be searched for\n",
        "    matcher.add('matcher_pattern', matcher_pattern)\n",
        "\n",
        "    # return the \"matcher\" objects; as Span objects(human readable text)\n",
        "    return matcher(self.doc, as_spans=True)\n",
        "\n",
        "def similar_outcome(str1, str2):\n",
        "    \"\"\"\n",
        "    Returns True if the strings are off by a single character, and that\n",
        "    character is not a 'd' at the end. That 'd' at the end of a word is highly\n",
        "    indicative of whether something is actually an outcome.\n",
        "\n",
        "    This is used in the get_outcome() method.\n",
        "    \"\"\"\n",
        "    if abs(len(str1) - len(str2)) > 1:\n",
        "        return False\n",
        "    min_len = min(len(str1), len(str2))\n",
        "    i = 0\n",
        "    while i < min_len and str1[i] == str2[i]:\n",
        "        i += 1\n",
        "\n",
        "    # We've reached the end of one string, the other is one character longer\n",
        "    if i == min_len:\n",
        "        # If that character is a 'd', return False, otherwise True\n",
        "        if ((len(str1) > len(str2) and str1[-1] == 'd')\n",
        "            or (len(str2) > len(str1) and str2[-1] == 'd')):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    # We're looking at a substitution that is 'd' at the end\n",
        "    if (i == len(str1) -1 and len(str1) == len(str2)\n",
        "        and (str1[-1] == 'd' or str2[-1] == 'd')):\n",
        "        return False\n",
        "\n",
        "    # We're looking at a substitution other than 'd' at the end\n",
        "    if str1[i+1:] == str2[i+1:]:\n",
        "        return True\n",
        "\n",
        "    # We're in the middle, str1 has an extra character\n",
        "    if str1[i+1:] == str2[i:]:\n",
        "        return True\n",
        "    \n",
        "    # We're in the middle, str2 has an extra character\n",
        "    if str1[i:] == str2[i+1:]:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def in_parenthetical(match, doc):\n",
        "    '''\n",
        "    Checks for text wrapped in parathesis, and removes any\n",
        "    returned protected grounds if they we're wrapped in parenthesis\n",
        "    used in protected grounds in order to improve accuracy\n",
        "    '''\n",
        "    open_parens = 0\n",
        "    for i in range(match.end, len(doc)):\n",
        "        if doc[i].text == '(':\n",
        "            open_parens += 1\n",
        "        elif doc[i].text == ')':\n",
        "            if open_parens > 0:\n",
        "                open_parens -= 1\n",
        "            else:\n",
        "                return True\n",
        "        elif doc[i] in {'.', '?', '!'}:\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "\"\"\"\n",
        "LISTS\n",
        "\n",
        "global info about judges; states and their court circuits\n",
        "\"\"\"\n",
        "\n",
        "# TODO: This static list should be stored and accessed via the backend\n",
        "appellate_panel_members = [\n",
        "     \"Adkins-Blanch, Charles K.\",\n",
        "     \"Michael P. Baird\",\n",
        "     \"Cassidy, William A.\",\n",
        "     \"Cole, Patricia A.\",\n",
        "     \"Couch, V. Stuart\",\n",
        "     \"Creppy, Michael J.\",\n",
        "     \"Crossett, John P.\",\n",
        "     \"Donovan, Teresa L.\",\n",
        "     \"Foote, Megan E.\",\n",
        "     \"Geller, Joan B.\",\n",
        "     \"Gemoets, Marcos\",\n",
        "     \"Gonzalez, Gabriel\",\n",
        "     \"Goodwin, Deborah K.\",\n",
        "     \"Gorman, Stephanie E.\",\n",
        "     \"Grant, Edward R.\",\n",
        "     \"Greer, Anne J.\",\n",
        "     \"Guendelsberger, John\",\n",
        "     \"Hunsucker, Keith E.\",\n",
        "     \"Kelly, Edward F.\",\n",
        "     \"Kendall Clark, Molly\",\n",
        "     \"Liebmann, Beth S.\",\n",
        "     \"Liebowitz, Ellen C.\",\n",
        "     \"Mahtabfar, Sunita B.\",\n",
        "     \"Malphrus, Garry D.\",\n",
        "     \"Mann, Ana\",\n",
        "     \"Miller, Neil P.\",\n",
        "     \"Monsky, Megan Foote\",\n",
        "     \"Montante Jr., Phillip J.\",\n",
        "     \"Morris, Daniel\",\n",
        "     \"Mullane, Hugh G.\",\n",
        "     \"Neal, David L.\",\n",
        "     \"Noferi, Mark\",\n",
        "     \"O'Connor, Blair\",\n",
        "     \"O'Herron, Margaret M.\",\n",
        "     \"O'Leary, Brian M.\",\n",
        "     \"Owen, Sirce E.\",\n",
        "     \"Pauley, Roger\",\n",
        "     \"Petty, Aaron R.\",\n",
        "     \"Pepper, S. Kathleen\",\n",
        "     \"RILEY, KEVIN W.\",\n",
        "     \"Rosen, Scott\",\n",
        "     \"Snow, Thomas G.\",\n",
        "     \"Swanwick, Daniel L.\",\n",
        "     \"Wendtland, Linda S.\",\n",
        "     \"Wetmore, David H.\",\n",
        "     \"Wilson, Earle B.\"\n",
        " ]\n",
        "\n",
        "\n",
        "'''used by get_circuit'''\n",
        "circuit_dict = {\n",
        "    'DC': 'DC', 'ME': '1', 'MA': '1', 'NH': '1', 'RI': '1', 'PR': '1',\n",
        "    'CT': '2', 'NY': '2', 'VT': '2', 'DE': '3', 'PA': '3', 'NJ': '3', 'VI': '3',\n",
        "    'MD': '4', 'VA': '4', 'NC': '4', 'SC': '4', 'WV': '4', 'LA': '5', 'MS': '5',\n",
        "    'TX': '5', 'KY': '6', 'OH': '6', 'TN': '6', 'MI': '6', 'IL': '7', 'IN': '7',\n",
        "    'WI': '7', 'AR': '8', 'IA': '8', 'MN': '8', 'MO': '8', 'NE': '8', 'ND': '8',\n",
        "    'SD': '8', 'AK': '9', 'AZ': '9', 'CA': '9', 'GU': '9', 'HI': '9', 'ID': '9',\n",
        "    'MT': '9', 'NV': '9', 'MP': '9', 'OR': '9', 'WA': '9', 'CO': '10',\n",
        "    'KS': '10', 'NM': '10', 'OK': '10', 'UT': '10', 'WY': '10', 'AL': '11',\n",
        "    'FL': '11', 'GA': '11'\n",
        " }\n",
        "\n",
        "\"\"\"CLASS and Get methods\n",
        "\n",
        "The following defines the BIACase Class. When receiving a court doc, we use this\n",
        "Class and the algorithms/methods to extract info for the desired fields/info\n",
        "that are scraped from the text of the court docs.\n",
        "\"\"\"\n",
        "\n",
        "class BIACase:\n",
        "    def __init__(self, text: str):\n",
        "        \"\"\"\n",
        "        • Input will be text from a BIA case pdf file, after the pdf has\n",
        "        been converted from PDF to text.\n",
        "        • Scraping works utilizing spaCy, tokenizing the text, and iterating\n",
        "        token by token searching for matching keywords.\n",
        "        \"\"\"\n",
        "        self.doc: Doc = nlp(text)\n",
        "        self.ents: Tuple[Span] = self.doc.ents\n",
        "        self.panel = self.get_panel(),\n",
        "        #get_panel needs to be called before is_appellate\n",
        "        self.appellate = self.is_appellate(),\n",
        "        self.application = self.get_application(),\n",
        "        self.date = self.get_date(),\n",
        "        self.country_of_origin = self.get_country_of_origin(),\n",
        "        # !!!get_outcome() needs to be called before get_protected_grounds()\n",
        "        self.outcome = self.get_outcome(),\n",
        "        # !!!get_state() needs to be called before get_city() and get_circuit()\n",
        "        self.state = self.get_state(),\n",
        "        self.city = self.get_city()\n",
        "        self.circuit = self.get_circuit()\n",
        "        self.protected_grounds = self.get_protected_grounds(),\n",
        "        self.based_violence = self.get_based_violence(),\n",
        "        self.gender = self.get_gender(),\n",
        "        self.indigenous_status = self.get_indigenous_status(),\n",
        "        self.applicant_language = self.get_applicant_language(),\n",
        "        self.credibility = self.get_credibility(),\n",
        "        self.one_year = self.check_for_one_year_new(),\n",
        "        self.precedent_cases = self.get_precedent_cases(),\n",
        "        self.statutes = self.get_statutes(),\n",
        "\n",
        "    def get_ents(self, labels: List[str]) -> Iterator[Span]:\n",
        "        \"\"\"\n",
        "        • Retrieves entitiess of a specified label(s) in the document,\n",
        "        if no label is specified, returns all entities\n",
        "        \"\"\"\n",
        "        return (ent for ent in self.ents if ent.label_ in labels)\n",
        "\n",
        "    def get_circuit(self):\n",
        "        '''returns the circuit the case started in.'''\n",
        "        return circuit_dict.get(self.state)\n",
        "\n",
        "    def is_appellate(self):\n",
        "\n",
        "            if self.panel:\n",
        "                return True\n",
        "\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "    def get_country_of_origin(self):\n",
        "        \"\"\"\n",
        "        RETURNS the respondent's or respondents' country of origin:\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"Generate COUNTRIES list\"\"\"\n",
        "        # sorted list of all current countries\n",
        "        gc = geonamescache.GeonamesCache()\n",
        "        countries = sorted(gc.get_countries_by_names().keys())\n",
        "        # remove U.S. and its territories from countries\n",
        "        if \"American Samoa\" in countries:\n",
        "            countries.remove(\"American Samoa\")\n",
        "        if \"Guam\" in countries:\n",
        "            countries.remove(\"Guam\")\n",
        "        if \"Northern Mariana Islands\" in countries:\n",
        "            countries.remove(\"Northern Mariana Islands\")\n",
        "        if \"Puerto Rico\" in countries:\n",
        "            countries.remove(\"Puerto Rico\")\n",
        "        if \"United States\" in countries:\n",
        "            countries.remove(\"United States\")\n",
        "        if \"United States Minor Outlying Islands\" in countries:\n",
        "            countries.remove(\"United States Minor Outlying Islands\")\n",
        "        if \"U.S. Virgin Islands\" in countries:\n",
        "            countries.remove(\"U.S. Virgin Islands\")\n",
        "\n",
        "        \"\"\"\n",
        "        PRIMARY search:\n",
        "        in most cases, the term/pattern \"citizen(s) of\" appears in the same\n",
        "            sentence the country of origin spacy.matcher patterns list, looking\n",
        "            for the following phrase matches following these patterns is\n",
        "            practically guaranteed to be the country of origin\n",
        "        \"\"\"\n",
        "        # create a spacy matcher pattern\n",
        "        primary_pattern = [\n",
        "            [{\"LOWER\": \"citizen\"}, {\"LOWER\": \"of\"}],\n",
        "            [{\"LOWER\": \"citizens\"}, {\"LOWER\": \"of\"}],\n",
        "        ]\n",
        "        # instantiate a list of pattern matches\n",
        "        spans = similar(self.doc, primary_pattern)\n",
        "        # if there are matches\n",
        "        if spans:\n",
        "            # grab the surrounding sentence and turn it into a string\n",
        "            sentence = str(spans[0].sent)\n",
        "            # remove line breaks, edge case\n",
        "            clean_sent = sentence.replace(\"\\n\", \" \")\n",
        "            # iterate through the countries list, and return it if it's in the\n",
        "                # cleaned sentence\n",
        "            for country in countries:\n",
        "                if country in clean_sent:\n",
        "                    return country\n",
        "\n",
        "        #SECONDARY search:\n",
        "        # If citizen of wasn't found or if it WAS found but no country followed,\n",
        "        # look through the whole doc for the first instance of a non-U.S. country.\n",
        "        else:\n",
        "            # untokenize and normalize\n",
        "            tok_text = str(self.doc).lower()\n",
        "            # edge case where line breaks appear in the middle of a multi-word\n",
        "                # country, an effect of turning the tokenized text to a string\n",
        "            clean_text = tok_text.replace(\"\\n\", \" \")\n",
        "            # iterate through countries for a foreign entity.\n",
        "            for country in countries:\n",
        "                if country.lower() in clean_text:\n",
        "                    return country\n",
        "\n",
        "    def get_date(self) -> str:\n",
        "        \"\"\"\n",
        "        • Returns date of the document. Easy to validate by the PDF filename.\n",
        "        \"\"\"\n",
        "        dates = map(str, self.get_ents(['DATE']))\n",
        "        for s in dates:\n",
        "            if len(s.split()) == 3:\n",
        "                return s\n",
        "\n",
        "    def get_panel(self):\n",
        "        \"\"\"\n",
        "        Uses the appellate_panel_members list and spacy PhraseMatcher to check a\n",
        "        document for members in the appellate_panel_member list.\n",
        "        !!! Currently only works for this static list of judges. If not appelate\n",
        "            or the list of apppelate judges changes, or there's an appelate\n",
        "            judge not in the list.\n",
        "            May want to generate an updatable list.\n",
        "            May want to generate a non-appellate judge list\n",
        "            This has important interactions with \"is_appellate()\" function. If\n",
        "                this function returns a judge, it IS from the appellate list,\n",
        "                and is therefore an appellate case.\n",
        "        \"\"\"\n",
        "         # create PhraseMatcher object with en_cor_web_md\n",
        "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "        # generate a list of patterns\n",
        "        patterns = [nlp.make_doc(text) for text in appellate_panel_members]\n",
        "        # add the patterns to the matcher's library\n",
        "        matcher.add(\"panel_names\", patterns)\n",
        "\n",
        "        # Create a list for phrase matches(judge names) to go.\n",
        "        matches = []\n",
        "        # find all matches, anywhere in the doc, that is a name from the\n",
        "        # appellate panel members list, and append them as spans(human readable)\n",
        "        for match_id, start, end in matcher(self.doc):\n",
        "            # to matches list\n",
        "            span = self.doc[start:end]\n",
        "            matches.append(span.text)\n",
        "        # if matches were found, return the list of panel members\n",
        "        if matches:\n",
        "            return sorted(set(matches))\n",
        "        # otherwise, the judge is in the appellate list; either not in the list\n",
        "            # or not an appellate judge\n",
        "        else:\n",
        "            return [] # IS THIS OKAY?\n",
        "\n",
        "    def get_gender(self):\n",
        "        \"\"\"\n",
        "        Searches through a given document and counts the TOTAL number of\n",
        "        \"male\" pronoun uses and \"female\" pronoun uses. Whichever\n",
        "        count(\"M\" or \"F\") is higher, that gender is returned.\n",
        "        In the event of a tie; currently returns \"Unknown\"; may be able to\n",
        "        code this edge case. Current accuracy is >95%, low priority fix.\n",
        "        \"\"\"\n",
        "        # List if gendered pronouns\n",
        "        male_prons = ['he', \"he's\", 'his', 'him', 'himself']\n",
        "        female_prons = ['she', \"she's\", 'her', 'hers', 'herself']\n",
        "\n",
        "        # list for spacy.matcher pattens\n",
        "        f_patterns = []\n",
        "        m_patterns = []\n",
        "\n",
        "        # generating list of patterns (f: pattern = [{'LOWER': word}]), for\n",
        "            # spacy matcher search\n",
        "\n",
        "        for prons in female_prons:\n",
        "            f_patterns.append([{'LOWER':prons}])\n",
        "        for prons in male_prons:\n",
        "            m_patterns.append([{'LOWER':prons}])\n",
        "\n",
        "        # use simlar() function (Above) to find patterns(pronouns) in whole\n",
        "        # text\n",
        "        m_similar = similar(self.doc, m_patterns)\n",
        "        f_similar = similar(self.doc, f_patterns)\n",
        "\n",
        "        # check the number of gendered pronoun occurrences and return gender\n",
        "        if len(m_similar) > len(f_similar):\n",
        "            return 'Male'\n",
        "        elif len(f_similar) > len(m_similar):\n",
        "            return 'Female'\n",
        "        else:\n",
        "            return 'Unknown'\n",
        "\n",
        "\n",
        "    def get_protected_grounds(self):\n",
        "        \"\"\"\n",
        "        This will return the protected ground(s) of the applicant. Special\n",
        "        checks are needed. Checking for keywords is not enough, as sometimes\n",
        "        documents label laws that describe each protected ground. Examples\n",
        "        are 'Purely Political Offense' and 'Real Id Act'.\n",
        "        \"\"\"\n",
        "        pattern = [\n",
        "        [{\"LOWER\": \"race\"}],\n",
        "        [{\"LOWER\": \"religion\"}], # expand to check for list of religions\n",
        "        [{\"LOWER\": \"nationality\"}], # phrase is pulled but out of context\n",
        "        [{\"LOWER\": \"social\"}, {\"LOWER\": \"group\"}],\n",
        "        [{\"LOWER\": \"political\"}, {\"LOWER\": \"opinion\"}],\n",
        "        [{\"LOWER\": \"political\"}, {\"LOWER\": \"offense\"}],\n",
        "        [{\"LOWER\": \"political\"}],\n",
        "        ]\n",
        "\n",
        "        religions = ['christianity','christian','islam','atheist','hinduism',\n",
        "                     'buddihism','jewish','judaism','islamist','sunni','shia',\n",
        "                     'muslim','buddhist','atheists','jew','hindu', 'atheism']\n",
        "\n",
        "        politicals = ['political opinion', 'political offense']\n",
        "\n",
        "        confirmed_matches = []\n",
        "        # create pattern for specified religions\n",
        "        for religion in religions:\n",
        "            pattern.append([{\"LOWER\": religion}])\n",
        "\n",
        "\n",
        "        potential_grounds = similar(self.doc, pattern)\n",
        "\n",
        "        for match in potential_grounds:\n",
        "            # remove 'nationality act' from potential_grounds\n",
        "            if match.text.lower() == 'nationality' \\\n",
        "            and 'act' not in match.sent.text.lower() \\\n",
        "            and 'nationality' not in confirmed_matches:\n",
        "                confirmed_matches.append('nationality')\n",
        "\n",
        "        # check for specified religion, replace with 'religion'\n",
        "            elif match.text.lower() in religions:\n",
        "                if 'religion' not in confirmed_matches:\n",
        "                    confirmed_matches.append('religion')\n",
        "\n",
        "            elif match.text.lower() in politicals:\n",
        "                if 'political' not in confirmed_matches:\n",
        "                    confirmed_matches.append('political')\n",
        "\n",
        "            else:\n",
        "                if match.text.lower() not in confirmed_matches:\n",
        "                    confirmed_matches.append(match.text.lower())\n",
        "        return confirmed_matches\n",
        "\n",
        "    def get_application(self) -> str:\n",
        "        \"\"\"\n",
        "        • This will return the seeker's application, found after 'APPLICATION'.\n",
        "        Because HRF is only interested in Asylum, Withholding of Removal,\n",
        "        and Convention Against Torture applications, the others should be\n",
        "        ignored and not included in the dataset.\n",
        "        \"\"\"\n",
        "        app_types = {\n",
        "            'CAT': ['Convention against Torture', 'Convention Against Torture'],\n",
        "            'Asylum': ['Asylum', 'asylum', 'asylum application'],\n",
        "            'Withholding of Removal': ['Withholding of Removal', 'withholding of removal'],\n",
        "            'Other': ['Termination', 'Reopening', \"Voluntary Departure\",\n",
        "                      'Cancellation of removal', 'Deferral of removal']\n",
        "        }\n",
        "\n",
        "        start = 0\n",
        "\n",
        "        for token in self.doc:\n",
        "            if token.text == 'APPLICATION':\n",
        "                start += token.idx\n",
        "                break\n",
        "\n",
        "        outcome = set()\n",
        "        for k, v in app_types.items():\n",
        "            for x in v:\n",
        "                if x in self.doc.text[start: start + 300]:\n",
        "                    if k == \"Other\":\n",
        "                        outcome.add(x)\n",
        "                    else:\n",
        "                        outcome.add(k)\n",
        "        return \"; \".join(list(outcome))\n",
        "\n",
        "    def get_outcome(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        • Returns list of outcome terms from the case in a list.\n",
        "          These will appear after 'ORDER' at the end of the document.\n",
        "        \"\"\"\n",
        "\n",
        "        outcomes_return = []\n",
        "        ordered_outcome = {'ORDER', 'ORDERED'}\n",
        "        outcomes_list = ['denied', 'dismissed', 'granted', 'remanded',\n",
        "                         'returned', 'sustained', 'terminated',\n",
        "                         'vacated', 'affirmed']\n",
        "        two_before_exclusion = {'may', 'any', 'has'}\n",
        "        one_before_exclusion = {'it', 'has'}\n",
        "\n",
        "        # locate where in the document the orders start\n",
        "        order_start_i = -1\n",
        "        for token in self.doc:\n",
        "            if token.text in ordered_outcome:\n",
        "                order_start_i = token.i\n",
        "                break\n",
        "\n",
        "        # If we can't find where the orders start, check the whole opinion\n",
        "        if order_start_i == -1:\n",
        "            order_start_i = 0\n",
        "\n",
        "        # Locate where in the document the orders end\n",
        "        order_end_i = len(self.doc)\n",
        "        # Orders end when we see \"FOR THE BOARD\" or \"WARNING\"\n",
        "        # - this avoids finding keywords in footnotes or warnings\n",
        "        for i in range(order_start_i+1, min(order_end_i, len(self.doc) - 2)):\n",
        "            if (self.doc[i:i+3].text == \"FOR THE BOARD\" or\n",
        "                self.doc[i].text == \"WARNING\"):\n",
        "                order_end_i = i\n",
        "                break\n",
        "\n",
        "        # Check the range for each type of outcome\n",
        "        for outcome in outcomes_list:\n",
        "            for i in range(order_start_i, order_end_i):\n",
        "                if (similar_outcome(self.doc[i].text, outcome) and\n",
        "                    self.doc[i-2].text not in two_before_exclusion and\n",
        "                    self.doc[i-1].text not in one_before_exclusion):\n",
        "                    outcomes_return.append(outcome)\n",
        "                    break\n",
        "\n",
        "        return outcomes_return\n",
        "\n",
        "    def get_state(doc):\n",
        "        #Create list to hold matcher patterns from state dictionary\n",
        "        state_patterns = []\n",
        "        \n",
        "        for k in court_locs.keys():\n",
        "            state_patterns.append([{\"LOWER\": k.lower()}])\n",
        "        \n",
        "        #create matcher pattern for the key phrase file\n",
        "        file_pattern = [\n",
        "            [{\"LOWER\": 'file'}],\n",
        "            [{\"LOWER\": 'files'}]\n",
        "        ]\n",
        "        \n",
        "        #instantiate file pattern using the doc, this returns a span object that lists all the words pulled from the doc and their sentence and the index positions of the start of that sentence and end\n",
        "        file_matches = similar(doc, file_pattern) \n",
        "        \n",
        "        #if spacy finds 'file' or 'files' we can grab the sentence that it contains. if not we can search the entire document for the state abbrev\n",
        "        if file_matches:\n",
        "            file_sentence = nlp(str(file_matches[0].sent))\n",
        "            #searches the sentence containing file for a state abbrev and returns the first one listed\n",
        "            state_matches = similar(file_sentence, state_patterns)\n",
        "            if state_matches:\n",
        "                return state_matches[0].text\n",
        "\n",
        "            #if file\n",
        "            else:\n",
        "                #if you change this output you need to change get_city to reflect that change.\n",
        "                return \"Please select state\"\n",
        "        \n",
        "        else:\n",
        "            #Searches entire document for states using matcher, returns them as a list that we can index. \n",
        "            state_matches = similar(doc, state_patterns)\n",
        "            if state_matches[0].text in court_locs.keys():\n",
        "                return state_matches[0]\n",
        "            else:\n",
        "                return \"No Matches\"\n",
        "            \n",
        "\n",
        "    def get_city(self):\n",
        "        \"\"\"\n",
        "        This function uses the get_state to filter the potential cities to find, then looks for the keywords file or files. \n",
        "            In most cases the city comes right after the keyword. \n",
        "        If the keyword isn't there it then creates a new matcher search for the entire corpus that finds the first city. \n",
        "            This needs to be revised since it will return Falls Church more than it should\n",
        "         \n",
        "        \"\"\"\n",
        "        #uses the abbreviation from self.state and #gets the list of cities in a state\n",
        "        citycache = []\n",
        "        city_pattern = []\n",
        "        # if get_state() function was unable to find a state it will return \"Please select state\". If you change this return in get_state you have to change this phrase here to correspond.\n",
        "        if self.state == \"Please select state\":\n",
        "            for i in court_locs.keys():\n",
        "                temp = court_locs.get(i)['city']\n",
        "                citycache.append(temp)\n",
        "            \n",
        "            \n",
        "        #if get_state() function does return a state this function will filter the court_loc dictionary by that state and return the corresponding cities.\n",
        "        else:\n",
        "            if self.state in court_locs.keys():\n",
        "                temp = court_locs.get(self.state)['city']\n",
        "                citycache.append(temp)\n",
        "            else:\n",
        "                for i in court_locs.keys():\n",
        "                    temp = court_locs.get(i)['city']\n",
        "                    citycache.append(temp)\n",
        "\n",
        "        #The length of this list is at most 72, and most of the time will be less than 15. The time complexity is negligble. This creates a spacy.matcher pattern \n",
        "        for i in citycache:\n",
        "            city_pattern.append([{\"LOWER\":i}])\n",
        "        \n",
        "        #The court location is almost always in the sentence following these two phrases. \n",
        "        pattern = [\n",
        "            [{\"LOWER\": 'file'}],\n",
        "            [{\"LOWER\": 'files'}]\n",
        "        ]\n",
        "\n",
        "        #instantiation of the above pattern using spacy.matcher, this returns a list of span objects that notes the pattern returned and it's index location. \n",
        "        matches = similar(self.doc, pattern)\n",
        "        \n",
        "        #if no matches are found (file or files) this function searches the entire corpus for the citys in court_locs.\n",
        "        if not matches:\n",
        "            matches = similar(self.doc, city_pattern)\n",
        "\n",
        "        #The first matcher return sentence for either option is stored here. .sent grabs the sentence of that matcher pattern. \n",
        "        sentence = str(matches[0].sent)\n",
        "        clean_sentence = sentence.replace(',', ' ').replace('\\n', ' ').title()\n",
        "        \n",
        "        #finds first city that's in the citycache\n",
        "        for word in clean_sentence:\n",
        "            if word in citycache:\n",
        "                return word\n",
        "            else:\n",
        "                return citycache\n",
        "\n",
        "\n",
        "    def get_based_violence(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Returns a list of keyword buckets which indicate certain types of violence mentioned in a case,\n",
        "        current buckets are: Violence, Family, Gender, and Gangs.\n",
        "        These keywords can be changed in their respective lists, and an item being present in the list\n",
        "        means that the given type of violence is mentioned in the document.\n",
        "        \"\"\"\n",
        "\n",
        "        # Converts words to lemmas & inputs to nlp-list, then searches for matches in the text\n",
        "        def get_matches(input_list, topic, full_text):\n",
        "            temp_matcher = PhraseMatcher(full_text.vocab, attr=\"LEMMA\")\n",
        "            for n in range(0, len(input_list)):\n",
        "                input_list[n] = nlp(input_list[n])\n",
        "            temp_matcher.add(topic, input_list)\n",
        "            temp_matches = temp_matcher(full_text)\n",
        "            return temp_matches\n",
        "\n",
        "        # Lists of keywords that fall within a bucket to search for\n",
        "        terms_list = []\n",
        "        violent_list = ['abduct', 'abuse', 'assassinate', 'assault', 'coerce', 'exploit',\n",
        "                        'fear', 'harm', 'hurt', 'kidnap', 'kill', 'murder', 'persecute',\n",
        "                        'rape', 'scare', 'shoot', 'suffer', 'threat', 'torture']\n",
        "        family_list = ['child', 'daughter', 'family', 'husband', 'parent', 'partner', 'son', 'wife', 'woman']\n",
        "        gender_list = ['fgm', 'gay', 'gender', 'homosexual', 'homosexuality', 'lesbian', 'lgbt', 'lgbtq', 'lgbtqia',\n",
        "                       'queer', 'sexuality', 'transgender']\n",
        "        gang_list = ['cartel', 'gang', 'militia']\n",
        "\n",
        "        # Outputs a list of PhraseMatch occurrences for a given list of keywords\n",
        "        violence_match = get_matches(violent_list, 'Violent', self.doc)\n",
        "        family_match = get_matches(family_list, 'Family', self.doc)\n",
        "        gender_match = get_matches(gender_list, 'Gender', self.doc)\n",
        "        gang_match = get_matches(gang_list, 'Gang', self.doc)\n",
        "\n",
        "        # Printing full_text[judge_match2[0][1]:judge_match2[0][2]] gives word it matches on, can put in the [0] a\n",
        "        # for loop to see all matches\n",
        "        if len(violence_match) != 0:\n",
        "            terms_list.append('Violent')\n",
        "        if len(family_match) != 0:\n",
        "            terms_list.append('Family')\n",
        "        if len(gender_match) != 0:\n",
        "            terms_list.append('Gender')\n",
        "        if len(gang_match) != 0:\n",
        "            terms_list.append('Gang')\n",
        "        return terms_list\n",
        "\n",
        "    def get_precedent_cases(self) -> List[str]:\n",
        "        \"\"\"\"\n",
        "        Returns a list of court cases mentioned within the document, i.e. 'Matter of A-B-' and 'Urbina-Mejia v. Holder'\n",
        "         \"\"\"\n",
        "        # These lists were largely gotten through trial and error & don't work as well on non-GCP documents initially\n",
        "        # set rules were broken & hardcoded stops made most sense when considering punctuation & mis-reads by OCR\n",
        "        cases_with_dupes = []\n",
        "        ok_words = {'&', \"'\", ',', '-', '.', 'al', 'et', 'ex', 'n', 'rel'}\n",
        "        reverse_break_words = {'(', '(IJ', 'Cf', 'Compare', 'He', 'I&N', 'IN', 'In', 'Section', 'See', 'She', 'Under',\n",
        "                               'While', 'and', 'as', 'at', 'because', 'cf', 'e.g.', 'in', 'is', 'procedural', 'section',\n",
        "                               'see', 'was'}\n",
        "        break_words = {'(', '(IJ', ',', '.', '....', 'Compare', 'He', 'I&N', 'IN', 'In', 'Section', 'See', 'She',\n",
        "                       'Under', 'While', 'and', 'as', 'at', 'because', 'e.g.', 'in', 'is', 'procedural', 'section',\n",
        "                       'see', 'was'}\n",
        "        for token in self.doc:\n",
        "            # Append 'X v. Y' precedent case: k loop extracts X, l loop extracts Y\n",
        "            if str(token) == 'v.':\n",
        "                test_index = token.i\n",
        "                vs_start = 0\n",
        "                vs_end = 0\n",
        "                for k in range(test_index - 1, test_index - 15, -1):\n",
        "                    if (str(self.doc[k])[0].isupper() == False and str(self.doc[k]) not in ok_words):\n",
        "                        vs_start = k + 1\n",
        "                        break\n",
        "                    if (str(self.doc[k])[0].isupper() == True and str(self.doc[k]) in reverse_break_words):\n",
        "                        vs_start = k + 1\n",
        "                        break\n",
        "                for l in range(test_index, test_index + 15):\n",
        "                    if (str(self.doc[l]) == ',' or str(self.doc[l]).isnumeric() == True or str(\n",
        "                            self.doc[l]) in break_words):\n",
        "                        vs_end = l\n",
        "                        break\n",
        "                if str(self.doc[vs_start:vs_end]) not in cases_with_dupes:\n",
        "                    cases_with_dupes.append(str(self.doc[vs_start:vs_end]))\n",
        "\n",
        "            # Append 'Matter of X' precedent cases\n",
        "            if str(token) == 'Matter':\n",
        "                start_index = token.i\n",
        "                false_flag = False\n",
        "                end_index = 0\n",
        "                for j in range(start_index, start_index + 15):\n",
        "                    # OCR misclassifies 'Matter of Z-Z-O-' as 'Matter of 2-2-0' enough times to need to hardcode this in\n",
        "                    if str(self.doc[j]).isnumeric() == True:\n",
        "                        if (str(self.doc[j])) == '2' or (str(self.doc[j]) == '0'):\n",
        "                            continue\n",
        "                        else:\n",
        "                            end_index += 1\n",
        "                            break\n",
        "                    if (str(self.doc[j])) in break_words or (str(self.doc[j]).isnumeric() == True):\n",
        "                        end_index = j\n",
        "                        break\n",
        "                    if str(self.doc[j]) == ':':\n",
        "                        false_flag = True\n",
        "                        break\n",
        "                if not false_flag:\n",
        "                    temp_var = str(self.doc[start_index:end_index])\n",
        "                    if temp_var not in cases_with_dupes:\n",
        "                        cases_with_dupes.append(temp_var)\n",
        "        cases_with_dupes = sorted(cases_with_dupes)\n",
        "\n",
        "        # k loop removes incorrect suffixes, l loop removes incorrect prefixes & prevents duplicates from being added.\n",
        "        clean_cases = []\n",
        "        final_cases = []\n",
        "        for k in range(0, len(cases_with_dupes)):\n",
        "            if (cases_with_dupes[k][0:-1] not in clean_cases and cases_with_dupes[k][0:-1] != ''):\n",
        "                clean_cases.append(cases_with_dupes[k])\n",
        "        for l in range(0, len(clean_cases)):\n",
        "            if (clean_cases[l][0:2] == '. ' or clean_cases[l][0:2] == ', '):\n",
        "                if clean_cases[l][0:2] not in final_cases:\n",
        "                    final_cases.append(clean_cases[l][2:])\n",
        "            elif clean_cases[l][0:3] == '., ':\n",
        "                if clean_cases[l][0:2] not in final_cases:\n",
        "                    final_cases.append(clean_cases[l][3:])\n",
        "            elif clean_cases[l] not in final_cases:\n",
        "                final_cases.append(clean_cases[l])\n",
        "        return [s.replace('\\n', ' ').replace('  ', ' ') for s in final_cases]\n",
        "\n",
        "    def get_statutes(self) -> dict:\n",
        "        \"\"\"Returns statutes mentioned in a given .txt document as a dictionary: {\"\"\"\n",
        "        # Removes patterns with only words instead of numbers,\n",
        "        # and matches known statute patterns' shape using spacy to be added\n",
        "        not_in_test = {'Xxx', 'xxx', 'XXX'}\n",
        "        statutes_list = []\n",
        "        for token in self.doc:\n",
        "            word_shape_match = str(token.shape_)\n",
        "            if word_shape_match[0:3] not in not_in_test:\n",
        "\n",
        "                # Matches shape of statutes- these can have 3-4 prefixes, 0-2 suffixes,\n",
        "                # and extracts all subsections if there isn't a space between any of them\n",
        "                statute_shape_match = str(token.shape_).replace('x','d').replace('X','d')\n",
        "                if(statute_shape_match[0:4] == 'ddd(' or statute_shape_match[0:4] == 'ddd.' or statute_shape_match[0:5] == 'dddd.' or statute_shape_match[0:5] == 'dddd(' or statute_shape_match[0:6] == 'ddddd.' or statute_shape_match[0:6] == 'ddddd('):\n",
        "                    # Close any open parentheses & append if not already present\n",
        "                    has_open_paren = False\n",
        "                    temp_token3 = str(token)\n",
        "                    for i in range(0, len(temp_token3)):\n",
        "                        if temp_token3[i] == \"(\":\n",
        "                            has_open_paren = True\n",
        "                        if temp_token3[i] == \")\":\n",
        "                            has_open_paren = False\n",
        "                    if has_open_paren == True:\n",
        "                        temp_token3 = temp_token3 + ')'\n",
        "                    if temp_token3 not in statutes_list:\n",
        "                        statutes_list.append(temp_token3)\n",
        "        statutes_list = sorted(statutes_list)\n",
        "\n",
        "        # Creates a dictionary with the key being the type of statute,\n",
        "        # and value being the listed statutes determined by the first 4 numbers in a statute.\n",
        "        return_dict = {}\n",
        "        CFR = []\n",
        "        INA = []\n",
        "        other = []\n",
        "        USC = []\n",
        "        for j in range(0, len(statutes_list)):\n",
        "            if (statutes_list[j][0:4].isnumeric() == True):\n",
        "                if (int(statutes_list[j][0:4]) >= 1000 and int(statutes_list[j][0:4]) <= 1003):\n",
        "                    CFR.append(statutes_list[j])\n",
        "                    continue\n",
        "                elif (int(statutes_list[j][0:4]) >= 1100 and int(statutes_list[j][0:4]) <= 1999):\n",
        "                    USC.append(statutes_list[j])\n",
        "                    continue\n",
        "                else:\n",
        "                    other.append(statutes_list[j])\n",
        "                    continue\n",
        "            elif (statutes_list[j][0:3].isnumeric() == True):\n",
        "                if (int(statutes_list[j][0:3]) >= 100 and int(statutes_list[j][0:3]) <= 199):\n",
        "                    USC.append(statutes_list[j])\n",
        "                    continue\n",
        "                elif (int(statutes_list[j][0:3]) >= 200 and int(statutes_list[j][0:3]) <= 399):\n",
        "                    INA.append(statutes_list[j])\n",
        "                    continue\n",
        "                else:\n",
        "                    other.append(statutes_list[j])\n",
        "                    continue\n",
        "            else:\n",
        "                other.append(statutes_list[j])\n",
        "\n",
        "        return_dict[\"CFR\"] = CFR\n",
        "        return_dict[\"INA\"] = INA\n",
        "        return_dict[\"Other\"] = other\n",
        "        return_dict[\"USC\"] = USC\n",
        "        return return_dict\n",
        "\n",
        "\n",
        "\n",
        "    def get_indigenous_status(self) -> str:\n",
        "        \"\"\"\n",
        "        • If the term \"indigenous\" appears in the document, the field will return\n",
        "        the name of asylum seeker's tribe/nation/group. Currently, the field will return\n",
        "        the two tokens that precede \"indigenous;\" this method needs to be fine-tuned and\n",
        "        validated.\n",
        "        \"\"\"\n",
        "        # indigenous: List[str]\n",
        "        # indigenous = [\n",
        "        #     'indigenous'\n",
        "        # ]\n",
        "        # !!! Would need to change this code to use SIMLAR/spacy matcher\n",
        "            # INSTEAD of similar_in_list\n",
        "        # similar_indig: Callable[[str, float], Union[str, None]]\n",
        "        # similar_indig = similar_in_list(indigenous)\n",
        "        #\n",
        "        # for token in self.doc:\n",
        "        #\n",
        "        #     sent: str\n",
        "        #     sent = token.sent.text.lower()\n",
        "        #\n",
        "        #     s: Union[str, None]\n",
        "        #     s = similar_indig(token.text.lower(), 0.9)\n",
        "        #\n",
        "        #     if s == 'indigenous group':\n",
        "        #         prev_wrds = self.doc[[token.i-1, token.i-2]].text.lower()\n",
        "        #         # return the name of the specific group/nation\n",
        "        #         return prev_wrds\n",
        "        return \"Test\"\n",
        "\n",
        "    def get_applicant_language(self) -> str:\n",
        "        \"\"\"\n",
        "        • If the term \"native speaker\" appears in the document, the field will return\n",
        "        the asylum seeker's stated native language. Currently, the field will return\n",
        "        the two tokens that precede \"native speaker;\" this method needs to be fine-tuned and\n",
        "        validated.\n",
        "        \"\"\"\n",
        "        # for token in self.doc:\n",
        "        #\n",
        "        #     sent: str\n",
        "        #     sent = token.sent.text.lower()\n",
        "        #\n",
        "        #     s: Union[str, None]\n",
        "        #     s = similar_pg(token.text.lower(), 0.9)\n",
        "        #\n",
        "        #     if s == 'native speaker' or s == 'native speakers':\n",
        "        #         next_wrds = self.doc[[token.i+1, token.i+2]].text.lower()\n",
        "        #         return next_wrds\n",
        "        #\n",
        "        # return 'Ability to testify in English'\n",
        "        return \"Test\"\n",
        "\n",
        "    def get_access_interpeter(self) -> str:\n",
        "        \"\"\"\n",
        "        • If the terms \"interpreter\" or \"translator\" appear in the document,\n",
        "        the field will return whether the asylum seeker had access to an\n",
        "        interpreter during their hearings. Currently, the field's output is\n",
        "        dependent on occurrence of specific tokens in the document; this method\n",
        "        needs to be fine-tuned and validated.\n",
        "        \"\"\"\n",
        "        # for token in self.doc:\n",
        "        #\n",
        "        #     sent: str\n",
        "        #     sent = token.sent.text.lower()\n",
        "        #\n",
        "        #     s: Union[str, None]\n",
        "        #     s = similar_pg(token.text.lower(), 0.9)\n",
        "        #\n",
        "        #     if s == 'interpreter' or s == 'translator':\n",
        "        #         surrounding: Span\n",
        "        #         surrounding = self.get_surrounding_sents(token)\n",
        "        #\n",
        "        #         next_word = self.doc[token.i+1].text.lower()\n",
        "        #         if 'requested' in surrounding.text.lower() \\\n",
        "        #             and 'granted' in surrounding.text.lower():\n",
        "        #             return 'Had access'\n",
        "        #         elif 'requested' in surrounding.text.lower() \\\n",
        "        #             and 'was present' in surrounding.text.lower():\n",
        "        #             return 'Yes'\n",
        "        #         elif 'requested' in surrounding.text.lower() \\\n",
        "        #             and 'granted' not in surrounding.text.lower():\n",
        "        #             return 'No'\n",
        "        #         elif 'requested' in surrounding.text.lower() \\\n",
        "        #             and 'was present' in surrounding.text.lower():\n",
        "        #             return 'No'\n",
        "        return \"Test\"\n",
        "\n",
        "    def get_credibility(self) -> str:\n",
        "        \"\"\"\n",
        "        • Returns the judge's decision on whether the applicant is a credible witness.\n",
        "        Curently, the field's output is dependent on occurance of specific tokens\n",
        "        in the document; this method needs to be fine-tuned and validated.\n",
        "        \"\"\"\n",
        "        # credibility = [\n",
        "        #     'credible'\n",
        "        # ]\n",
        "        # !!! NO Longer using similar_in_list, Use the new SIMLIlAR function\n",
        "            # and SPACY matcher or PhraseMatcher\n",
        "        # similar_cred: Callable[[str, float], Union[str, None]]\n",
        "        # similar_cred = similar_in_list(credibility)\n",
        "        # for token in self.doc:\n",
        "        #     sent: str\n",
        "        #     sent = token.sent.text.lower()\n",
        "        #     s: Union[str, None]\n",
        "        #     s = similar_cred(token.text.lower(), 0.9)\n",
        "        #     if s == 'credible':\n",
        "        #         prev_word = self.doc[token.i-1].text.lower()\n",
        "        #         next_word = self.doc[token.i+1].text.lower()\n",
        "        #         if not similar(prev_word, 'not', 0.95) \\\n",
        "        #             and not similar(next_word, 'witness', 0.95):\n",
        "        #             return 'Yes'\n",
        "        #         else:\n",
        "        #             return 'No'\n",
        "        #         if s not in self.doc:\n",
        "        #             return 'N/A to case'\n",
        "        return \"Test\"\n",
        "\n",
        "    def check_for_one_year_new(self) -> bool:\n",
        "        \"\"\"\n",
        "        Checks whether or not the asylum-seeker argued to be exempt from the\n",
        "        one-year guideline.\n",
        "        \n",
        "        Returns true if the phrase \"within one-year\" appears in the document.\n",
        "        Also returns true if a time-based word appears in the same sentence\n",
        "        with \"extraordinary circumstances\" or \"changed circumstances\" or\n",
        "        \"untimely application\". Otherwise, returns False.\n",
        "        \n",
        "        \"\"\"\n",
        "        year_pattern = [\n",
        "            [{'LOWER':'within'}, {'LOWER': {'IN':['1', 'one']}}, \n",
        "             {'LOWER': '-', 'OP': '?'}, {'LOWER': 'year'}]\n",
        "        ]\n",
        "        matcher = Matcher(nlp.vocab)\n",
        "        matcher.add('year pattern', year_pattern)\n",
        "        matches = matcher(self.doc, as_spans=True)\n",
        "        if matches:\n",
        "            return True\n",
        "        matcher.remove('year pattern')\n",
        "        \n",
        "        secondary_terms = {'year', 'delay', 'time', 'period', 'deadline'}\n",
        "        circumstance_pattern = [\n",
        "            [{'LEMMA': {'IN':['change', 'extraordinary']}}, \n",
        "             {'LOWER': {'IN':['\"', '”']}, 'OP': '?'}, {'LEMMA': 'circumstance'}]\n",
        "        ]\n",
        "        application_pattern = [\n",
        "            [{'LOWER':'untimely'}, {'LOWER':'application'}]\n",
        "        ]\n",
        "        matcher.add('circumstance pattern', circumstance_pattern)\n",
        "        matcher.add('application pattern', application_pattern)\n",
        "        matches = matcher(self.doc, as_spans=True)\n",
        "        for match in matches:\n",
        "            for token in match.sent:\n",
        "                if token.lemma_ in secondary_terms:\n",
        "                    return True\n",
        "        return False\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HAo1WTJMx2ZK",
        "outputId": "7cd12fa2-3e67-4481-d98b-d67ed3d01503"
      },
      "source": [
        "#create the make_fields function here\n",
        "\n",
        "\"\"\"\n",
        "MAKE FIELDS\n",
        "This is the main overall function that creates a dictionary of the desired\n",
        "fields and their respective values; info that goes into those fields.\n",
        "\"\"\"\n",
        "\n",
        "#This text_dict functions as the database on AWS\n",
        "text_dict = {}\n",
        "\n",
        "\n",
        "def make_fields(incoming):\n",
        "  start = time.time()\n",
        "  case = BIACase(incoming)\n",
        "  case_data = {\n",
        "      'application': case.application,\n",
        "      'date': case.date,\n",
        "      'country of origin': case.country_of_origin,\n",
        "      'panel members': case.panel,\n",
        "      'outcome': case.outcome,\n",
        "      'state of origin': case.state,\n",
        "      'city of origin': case.city,\n",
        "      'circuit of origin': case.circuit,\n",
        "      'protected grounds': case.protected_grounds,\n",
        "      'based violence': case.based_violence,\n",
        "      'gender': case.gender,\n",
        "      'indigenous': case.indigenous_status,\n",
        "      'applicant language': case.applicant_language,\n",
        "      'credibility': case.credibility,\n",
        "      'check for one year': case.one_year,\n",
        "      'precedent cases': case.precedent_cases,\n",
        "      'statutes': case.statutes,\n",
        "    }\n",
        "  time_taken = time.time() - start\n",
        "  case_data[\"time to process\"] = f\"{time_taken:.2f} seconds\"\n",
        "  return case_data\n",
        "  \n",
        "\n",
        "\n",
        "#Loop through the files on the drive and creates an imbedded dictionary for each UUID. \n",
        "for file in filenames:\n",
        "  f = open(f\"/content/drive/MyDrive/Lambda/hrfCases/{file}\", 'r', encoding='utf-8')\n",
        "  text_dict[file] = make_fields(f.read())\n",
        "\n",
        "\n",
        "\n",
        "#To create a dataframe for this information you need to unpack the uuid as an index, then look at text_dict.values(), which is a layered dictionary. \n",
        "for k, v in text_dict.items():\n",
        "  df = pd.DataFrame.from_dict(text_dict, orient='index', columns = [\"application\", 'date', 'country_of_origin', 'panel_members', 'outcome', 'state_of_origin', 'city_of_origin', 'circuit_of_origin', 'protected_grounds', 'based_violence', 'gender', 'indigenous', 'applicant language', 'credibility', 'check_for_one_year', 'precedent_cases', 'statutes'])\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>application</th>\n",
              "      <th>date</th>\n",
              "      <th>country_of_origin</th>\n",
              "      <th>panel_members</th>\n",
              "      <th>outcome</th>\n",
              "      <th>state_of_origin</th>\n",
              "      <th>city_of_origin</th>\n",
              "      <th>circuit_of_origin</th>\n",
              "      <th>protected_grounds</th>\n",
              "      <th>based_violence</th>\n",
              "      <th>gender</th>\n",
              "      <th>indigenous</th>\n",
              "      <th>applicant language</th>\n",
              "      <th>credibility</th>\n",
              "      <th>check_for_one_year</th>\n",
              "      <th>precedent_cases</th>\n",
              "      <th>statutes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>351906329-Naser-Noaman-Mohamed-Al-Maotari-A077-251-699-BIA-June-2-2017-output-1-to-6.txt</th>\n",
              "      <td>(Asylum,)</td>\n",
              "      <td>(December 30, 2014,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded, sustained],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Male,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(A)(3)(i)'], 'INA': ['208(b)(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349320269-S-D-AXXX-XXX-230-BIA-April-26-2017-output-1-to-8.txt</th>\n",
              "      <td>(Withholding of Removal; Termination; CAT,)</td>\n",
              "      <td>(April 26, 2017,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([sustained, terminated],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Male,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322069426-Maricela-Jacquelin-Madrid-Gomez-A206-254-226-BIA-Aug-11-2016-output-1-to-7.txt</th>\n",
              "      <td>(Reopening,)</td>\n",
              "      <td>(Aug. 11, 2016,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded, sustained],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Female,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311736830-H-R-M-AXXX-XXX-381-BIA-March-14-2016-output-1-to-5.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum; CAT,)</td>\n",
              "      <td>(May 11, 2005,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded, sustained],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Female,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(c)', '1003.1(d)(3)(i)', '100...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202216334-Francisco-Hernandez-Pina-A073-976-639-BIA-Jan-19-2012-output-1-to-4.txt</th>\n",
              "      <td>(,)</td>\n",
              "      <td>(Jan. 19, 2012,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded, sustained],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Male,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(6)', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466549989-C-F-M-C-AXXX-XXX-962-BIA-May-13-2020-output-1-to-3.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum; CAT,)</td>\n",
              "      <td>(May 13, 2020,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Male,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(iv)'], 'INA': ['208(b)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477306913-D-P-J-AXXX-XXX-170-BIA-Aug-7-2020-output-1-to-4.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum; CAT,)</td>\n",
              "      <td>(Aug. 7, 2020,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Female,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481527271-D-G-G-AXXX-XXX-299-BIA-Sept-24-2020-output-1-to-4.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum,)</td>\n",
              "      <td>(Sept. 24, 2020),)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Female,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481527396-Y-H-AXXX-XXX-948-BIA-Sept-24-2020-output-1-to-3.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum; CAT,)</td>\n",
              "      <td>(Sept. 24, 2020),)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([remanded, sustained, vacated],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Female,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473851235-K-I-AXXX-XXX-553-BIA-Aug-5-2020-output-1-to-4.txt</th>\n",
              "      <td>(Withholding of Removal; Asylum; CAT,)</td>\n",
              "      <td>(Aug. 5, 2020,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>([dismissed, remanded],)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(Male,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>(Test,)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>({'CFR': ['1003.1(0)6', '1003.1(d)(3)(i)', '10...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>170 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                    application  ...                                           statutes\n",
              "351906329-Naser-Noaman-Mohamed-Al-Maotari-A077-...                                    (Asylum,)  ...  ({'CFR': ['1003.1(A)(3)(i)'], 'INA': ['208(b)(...\n",
              "349320269-S-D-AXXX-XXX-230-BIA-April-26-2017-ou...  (Withholding of Removal; Termination; CAT,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...\n",
              "322069426-Maricela-Jacquelin-Madrid-Gomez-A206-...                                 (Reopening,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...\n",
              "311736830-H-R-M-AXXX-XXX-381-BIA-March-14-2016-...       (Withholding of Removal; Asylum; CAT,)  ...  ({'CFR': ['1003.1(c)', '1003.1(d)(3)(i)', '100...\n",
              "202216334-Francisco-Hernandez-Pina-A073-976-639...                                          (,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(6)', '...\n",
              "...                                                                                         ...  ...                                                ...\n",
              "466549989-C-F-M-C-AXXX-XXX-962-BIA-May-13-2020-...       (Withholding of Removal; Asylum; CAT,)  ...  ({'CFR': ['1003.1(d)(3)(iv)'], 'INA': ['208(b)...\n",
              "477306913-D-P-J-AXXX-XXX-170-BIA-Aug-7-2020-out...       (Withholding of Removal; Asylum; CAT,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...\n",
              "481527271-D-G-G-AXXX-XXX-299-BIA-Sept-24-2020-o...            (Withholding of Removal; Asylum,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...\n",
              "481527396-Y-H-AXXX-XXX-948-BIA-Sept-24-2020-out...       (Withholding of Removal; Asylum; CAT,)  ...  ({'CFR': ['1003.1(d)(3)(i)', '1003.1(d)(3)(ii)...\n",
              "473851235-K-I-AXXX-XXX-553-BIA-Aug-5-2020-outpu...       (Withholding of Removal; Asylum; CAT,)  ...  ({'CFR': ['1003.1(0)6', '1003.1(d)(3)(i)', '10...\n",
              "\n",
              "[170 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}